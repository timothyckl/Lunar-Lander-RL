{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import wandb\n",
    "import gymnasium as gym\n",
    "from models.ddql import DDQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\p2106911/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 5f71bc6f91cdaa551a70e88cf2522fcc1425d29b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimothyckl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\p2106911\\Desktop\\Reinforcement-Learning-CA2\\wandb\\run-20230125_143055-thf0b3cg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/thf0b3cg\" target=\"_blank\">lunar-peony-45</a></strong> to <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl\" target=\"_blank\">https://wandb.ai/timothyckl/lunar-lander-rl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/thf0b3cg\" target=\"_blank\">https://wandb.ai/timothyckl/lunar-lander-rl/runs/thf0b3cg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/thf0b3cg?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x23d67d936d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', continuous=False, render_mode='rgb_array')\n",
    "learning_rate = 0.0005\n",
    "discount_factor = 0.99\n",
    "exploration_rate = 1.0\n",
    "episodes = 500\n",
    "max_steps = 1000\n",
    "\n",
    "wandb.init(project='lunar-lander-rl', entity='timothyckl', config={\n",
    "    'learning_rate': learning_rate,\n",
    "    'discount_factor': discount_factor,\n",
    "    'exploration_rate': exploration_rate,\n",
    "    'episodes': episodes,\n",
    "    'max_steps': max_steps\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 1/500] - Reward: -286.8276 - Steps: 77 - Eps: 0.8687 - Time: 4.37s\n",
      "[EP 2/500] - Reward: -66.0631 - Steps: 181 - Eps: 0.1409 - Time: 36.34s\n",
      "[EP 3/500] - Reward: -255.1007 - Steps: 157 - Eps: 0.0291 - Time: 33.08s\n",
      "[EP 4/500] - Reward: -354.1115 - Steps: 428 - Eps: 0.0100 - Time: 88.31s\n",
      "[EP 5/500] - Reward: -485.1165 - Steps: 77 - Eps: 0.0100 - Time: 16.07s\n",
      "[EP 6/500] - Reward: -363.9022 - Steps: 140 - Eps: 0.0100 - Time: 28.68s\n",
      "[EP 7/500] - Reward: -289.3213 - Steps: 108 - Eps: 0.0100 - Time: 22.17s\n",
      "[EP 8/500] - Reward: -189.2141 - Steps: 155 - Eps: 0.0100 - Time: 32.40s\n",
      "[EP 9/500] - Reward: -79.5882 - Steps: 95 - Eps: 0.0100 - Time: 19.75s\n",
      "[EP 10/500] - Reward: -133.0447 - Steps: 566 - Eps: 0.0100 - Time: 116.27s\n",
      "[EP 11/500] - Reward: -133.5487 - Steps: 318 - Eps: 0.0100 - Time: 65.43s\n",
      "[EP 12/500] - Reward: 194.0888 - Steps: 427 - Eps: 0.0100 - Time: 89.86s\n",
      "[EP 13/500] - Reward: -157.7037 - Steps: 459 - Eps: 0.0100 - Time: 96.11s\n",
      "[EP 14/500] - Reward: -47.3045 - Steps: 247 - Eps: 0.0100 - Time: 51.37s\n",
      "[EP 15/500] - Reward: 39.4932 - Steps: 116 - Eps: 0.0100 - Time: 23.99s\n",
      "[EP 16/500] - Reward: -67.3570 - Steps: 185 - Eps: 0.0100 - Time: 38.74s\n",
      "[EP 17/500] - Reward: 4.2036 - Steps: 127 - Eps: 0.0100 - Time: 26.34s\n",
      "[EP 18/500] - Reward: -125.0624 - Steps: 128 - Eps: 0.0100 - Time: 26.87s\n",
      "[EP 19/500] - Reward: -136.1159 - Steps: 220 - Eps: 0.0100 - Time: 45.91s\n",
      "[EP 20/500] - Reward: -118.2751 - Steps: 298 - Eps: 0.0100 - Time: 62.16s\n",
      "[EP 21/500] - Reward: 148.6829 - Steps: 371 - Eps: 0.0100 - Time: 78.27s\n",
      "[EP 22/500] - Reward: -11.8522 - Steps: 1000 - Eps: 0.0100 - Time: 213.40s\n",
      "[EP 23/500] - Reward: 5.3749 - Steps: 1000 - Eps: 0.0100 - Time: 212.01s\n",
      "[EP 24/500] - Reward: -135.6893 - Steps: 227 - Eps: 0.0100 - Time: 48.50s\n",
      "[EP 25/500] - Reward: -64.4094 - Steps: 1000 - Eps: 0.0100 - Time: 213.47s\n",
      "[EP 26/500] - Reward: -119.2283 - Steps: 155 - Eps: 0.0100 - Time: 33.46s\n",
      "[EP 27/500] - Reward: -102.9653 - Steps: 137 - Eps: 0.0100 - Time: 29.40s\n",
      "[EP 28/500] - Reward: -71.5759 - Steps: 1000 - Eps: 0.0100 - Time: 214.79s\n",
      "[EP 29/500] - Reward: -17.1261 - Steps: 1000 - Eps: 0.0100 - Time: 216.20s\n",
      "[EP 30/500] - Reward: 25.6007 - Steps: 1000 - Eps: 0.0100 - Time: 220.50s\n",
      "[EP 31/500] - Reward: 0.8656 - Steps: 1000 - Eps: 0.0100 - Time: 226.78s\n",
      "[EP 32/500] - Reward: -24.9538 - Steps: 1000 - Eps: 0.0100 - Time: 238.64s\n",
      "[EP 33/500] - Reward: -43.8192 - Steps: 1000 - Eps: 0.0100 - Time: 243.06s\n",
      "[EP 34/500] - Reward: -46.7653 - Steps: 1000 - Eps: 0.0100 - Time: 246.70s\n",
      "[EP 35/500] - Reward: -42.0872 - Steps: 1000 - Eps: 0.0100 - Time: 256.14s\n",
      "[EP 36/500] - Reward: -11.9424 - Steps: 1000 - Eps: 0.0100 - Time: 268.10s\n",
      "[EP 37/500] - Reward: -32.9681 - Steps: 1000 - Eps: 0.0100 - Time: 273.29s\n",
      "[EP 38/500] - Reward: -51.8720 - Steps: 1000 - Eps: 0.0100 - Time: 283.29s\n",
      "[EP 39/500] - Reward: -4.4687 - Steps: 1000 - Eps: 0.0100 - Time: 291.03s\n",
      "[EP 40/500] - Reward: 2.4568 - Steps: 1000 - Eps: 0.0100 - Time: 299.85s\n",
      "[EP 41/500] - Reward: -129.5315 - Steps: 76 - Eps: 0.0100 - Time: 23.27s\n",
      "[EP 42/500] - Reward: -110.4115 - Steps: 92 - Eps: 0.0100 - Time: 28.44s\n",
      "[EP 43/500] - Reward: -81.9048 - Steps: 87 - Eps: 0.0100 - Time: 26.75s\n",
      "[EP 44/500] - Reward: -30.7451 - Steps: 138 - Eps: 0.0100 - Time: 41.52s\n",
      "[EP 45/500] - Reward: -21.4071 - Steps: 1000 - Eps: 0.0100 - Time: 291.62s\n",
      "[EP 46/500] - Reward: -63.5520 - Steps: 1000 - Eps: 0.0100 - Time: 304.16s\n",
      "[EP 47/500] - Reward: -79.9124 - Steps: 1000 - Eps: 0.0100 - Time: 320.74s\n",
      "[EP 48/500] - Reward: -62.2099 - Steps: 1000 - Eps: 0.0100 - Time: 345.38s\n",
      "[EP 49/500] - Reward: -5.9914 - Steps: 1000 - Eps: 0.0100 - Time: 352.96s\n",
      "[EP 50/500] - Reward: 44.7241 - Steps: 1000 - Eps: 0.0100 - Time: 351.76s\n",
      "[EP 51/500] - Reward: 101.7384 - Steps: 1000 - Eps: 0.0100 - Time: 352.99s\n",
      "[EP 52/500] - Reward: 237.5897 - Steps: 278 - Eps: 0.0100 - Time: 98.65s\n",
      "[EP 53/500] - Reward: 222.2210 - Steps: 223 - Eps: 0.0100 - Time: 79.41s\n",
      "[EP 54/500] - Reward: -2.2105 - Steps: 140 - Eps: 0.0100 - Time: 49.38s\n",
      "[EP 55/500] - Reward: 23.5416 - Steps: 162 - Eps: 0.0100 - Time: 56.60s\n",
      "[EP 56/500] - Reward: 2.2221 - Steps: 167 - Eps: 0.0100 - Time: 58.84s\n",
      "[EP 57/500] - Reward: 34.2772 - Steps: 170 - Eps: 0.0100 - Time: 59.90s\n",
      "[EP 58/500] - Reward: -24.8600 - Steps: 162 - Eps: 0.0100 - Time: 56.86s\n",
      "[EP 59/500] - Reward: 227.5506 - Steps: 317 - Eps: 0.0100 - Time: 112.72s\n",
      "[EP 60/500] - Reward: 202.7335 - Steps: 320 - Eps: 0.0100 - Time: 117.85s\n",
      "[EP 61/500] - Reward: 185.9750 - Steps: 407 - Eps: 0.0100 - Time: 149.35s\n",
      "[EP 62/500] - Reward: 209.6084 - Steps: 379 - Eps: 0.0100 - Time: 139.69s\n",
      "[EP 63/500] - Reward: 48.6038 - Steps: 1000 - Eps: 0.0100 - Time: 366.25s\n",
      "[EP 64/500] - Reward: -31.0167 - Steps: 1000 - Eps: 0.0100 - Time: 347.90s\n",
      "[EP 65/500] - Reward: 202.4366 - Steps: 273 - Eps: 0.0100 - Time: 100.07s\n",
      "[EP 66/500] - Reward: 49.4239 - Steps: 209 - Eps: 0.0100 - Time: 78.00s\n",
      "[EP 67/500] - Reward: 215.3175 - Steps: 504 - Eps: 0.0100 - Time: 183.86s\n",
      "[EP 68/500] - Reward: 111.3616 - Steps: 1000 - Eps: 0.0100 - Time: 378.24s\n",
      "[EP 69/500] - Reward: -16.4706 - Steps: 1000 - Eps: 0.0100 - Time: 400.77s\n",
      "[EP 70/500] - Reward: 270.9610 - Steps: 453 - Eps: 0.0100 - Time: 182.73s\n",
      "[EP 71/500] - Reward: 228.7162 - Steps: 823 - Eps: 0.0100 - Time: 330.68s\n",
      "[EP 72/500] - Reward: -86.5879 - Steps: 1000 - Eps: 0.0100 - Time: 403.18s\n",
      "[EP 73/500] - Reward: 167.3505 - Steps: 634 - Eps: 0.0100 - Time: 238.55s\n"
     ]
    }
   ],
   "source": [
    "agent = DDQL(env, learning_rate, discount_factor, exploration_rate)\n",
    "history = agent.train(episodes, max_steps, log_wandb=True, update=True, save_episodes=False, save_interval=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test agent\n",
    "# history = agent.train(10, 1000, log_wandb=False, update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history\n",
    "if not os.path.exists('./history'):\n",
    "        os.mkdir('./history')\n",
    "\n",
    "with open('./history/ddql_history.json', 'w') as file:\n",
    "    json.dump(history, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
