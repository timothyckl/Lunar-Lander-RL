{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import wandb\n",
    "import gymnasium as gym\n",
    "from models.ddql import DDQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\timot/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 5f71bc6f91cdaa551a70e88cf2522fcc1425d29b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimothyckl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\timot\\Desktop\\DELE_CA2\\Reinforcement-Learning-CA2\\wandb\\run-20230124_143746-hsl2fiyo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/hsl2fiyo\" target=\"_blank\">bright-dragon-23</a></strong> to <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl\" target=\"_blank\">https://wandb.ai/timothyckl/lunar-lander-rl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/hsl2fiyo\" target=\"_blank\">https://wandb.ai/timothyckl/lunar-lander-rl/runs/hsl2fiyo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/hsl2fiyo?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x254d44aac70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', continuous=False, render_mode='rgb_array')\n",
    "learning_rate = 0.0005\n",
    "discount_factor = 0.99\n",
    "exploration_rate = 1.0\n",
    "episodes = 100\n",
    "max_steps = 1000\n",
    "\n",
    "wandb.init(project='lunar-lander-rl', entity='timothyckl', config={\n",
    "    'learning_rate': learning_rate,\n",
    "    'discount_factor': discount_factor,\n",
    "    'exploration_rate': exploration_rate,\n",
    "    'episodes': episodes,\n",
    "    'max_steps': max_steps\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 1/100] - Reward: -112.2584 - Steps: 92 - Eps: 0.9900 - Time: 8.76s\n",
      "[EP 2/100] - Reward: -137.9520 - Steps: 91 - Eps: 0.9801 - Time: 28.82s\n",
      "[EP 3/100] - Reward: -434.7305 - Steps: 117 - Eps: 0.9703 - Time: 37.17s\n",
      "[EP 4/100] - Reward: -177.9919 - Steps: 65 - Eps: 0.9606 - Time: 21.30s\n",
      "[EP 5/100] - Reward: -207.3846 - Steps: 119 - Eps: 0.9510 - Time: 37.63s\n",
      "[EP 6/100] - Reward: -97.4933 - Steps: 64 - Eps: 0.9415 - Time: 20.10s\n",
      "[EP 7/100] - Reward: -175.1908 - Steps: 94 - Eps: 0.9321 - Time: 28.96s\n",
      "[EP 8/100] - Reward: -123.7325 - Steps: 78 - Eps: 0.9227 - Time: 25.94s\n",
      "[EP 9/100] - Reward: -193.3415 - Steps: 75 - Eps: 0.9135 - Time: 23.39s\n",
      "[EP 10/100] - Reward: -92.5677 - Steps: 98 - Eps: 0.9044 - Time: 31.36s\n",
      "[EP 11/100] - Reward: -226.6868 - Steps: 82 - Eps: 0.8953 - Time: 26.84s\n",
      "[EP 12/100] - Reward: -207.6520 - Steps: 93 - Eps: 0.8864 - Time: 28.96s\n",
      "[EP 13/100] - Reward: -429.0988 - Steps: 75 - Eps: 0.8775 - Time: 24.42s\n",
      "[EP 14/100] - Reward: -461.6120 - Steps: 104 - Eps: 0.8687 - Time: 32.20s\n",
      "[EP 15/100] - Reward: -172.4997 - Steps: 98 - Eps: 0.8601 - Time: 31.58s\n",
      "[EP 16/100] - Reward: -101.4497 - Steps: 72 - Eps: 0.8515 - Time: 22.14s\n",
      "[EP 17/100] - Reward: -164.8941 - Steps: 102 - Eps: 0.8429 - Time: 33.31s\n",
      "[EP 18/100] - Reward: -92.9726 - Steps: 82 - Eps: 0.8345 - Time: 25.89s\n",
      "[EP 19/100] - Reward: -111.1607 - Steps: 108 - Eps: 0.8262 - Time: 33.75s\n",
      "[EP 20/100] - Reward: -86.6384 - Steps: 129 - Eps: 0.8179 - Time: 40.18s\n",
      "[EP 21/100] - Reward: -242.4178 - Steps: 108 - Eps: 0.8097 - Time: 34.19s\n",
      "[EP 22/100] - Reward: -156.3494 - Steps: 120 - Eps: 0.8016 - Time: 38.42s\n",
      "[EP 23/100] - Reward: -180.9326 - Steps: 151 - Eps: 0.7936 - Time: 53.86s\n",
      "[EP 24/100] - Reward: -105.3770 - Steps: 101 - Eps: 0.7857 - Time: 37.04s\n",
      "[EP 25/100] - Reward: -78.5625 - Steps: 88 - Eps: 0.7778 - Time: 30.88s\n",
      "[EP 26/100] - Reward: -131.9593 - Steps: 100 - Eps: 0.7700 - Time: 35.50s\n",
      "[EP 27/100] - Reward: -210.2640 - Steps: 95 - Eps: 0.7623 - Time: 34.13s\n",
      "[EP 28/100] - Reward: -65.5794 - Steps: 59 - Eps: 0.7547 - Time: 20.69s\n",
      "[EP 29/100] - Reward: -87.5044 - Steps: 99 - Eps: 0.7472 - Time: 35.29s\n",
      "[EP 30/100] - Reward: -72.7978 - Steps: 87 - Eps: 0.7397 - Time: 31.09s\n",
      "[EP 31/100] - Reward: -81.8513 - Steps: 120 - Eps: 0.7323 - Time: 42.22s\n",
      "[EP 32/100] - Reward: -70.6318 - Steps: 116 - Eps: 0.7250 - Time: 41.77s\n",
      "[EP 33/100] - Reward: -99.1139 - Steps: 104 - Eps: 0.7177 - Time: 36.35s\n",
      "[EP 34/100] - Reward: -18.9940 - Steps: 87 - Eps: 0.7106 - Time: 31.53s\n",
      "[EP 35/100] - Reward: -22.2287 - Steps: 99 - Eps: 0.7034 - Time: 34.54s\n",
      "[EP 36/100] - Reward: -151.9873 - Steps: 100 - Eps: 0.6964 - Time: 36.24s\n",
      "[EP 37/100] - Reward: -160.9551 - Steps: 115 - Eps: 0.6894 - Time: 41.14s\n",
      "[EP 38/100] - Reward: -107.5053 - Steps: 115 - Eps: 0.6826 - Time: 42.21s\n",
      "[EP 39/100] - Reward: -34.2895 - Steps: 66 - Eps: 0.6757 - Time: 23.31s\n",
      "[EP 40/100] - Reward: -94.8633 - Steps: 96 - Eps: 0.6690 - Time: 34.47s\n",
      "[EP 41/100] - Reward: -92.9340 - Steps: 102 - Eps: 0.6623 - Time: 35.91s\n",
      "[EP 42/100] - Reward: -73.7178 - Steps: 134 - Eps: 0.6557 - Time: 47.93s\n",
      "[EP 43/100] - Reward: -189.4962 - Steps: 116 - Eps: 0.6491 - Time: 42.72s\n",
      "[EP 44/100] - Reward: -99.0221 - Steps: 125 - Eps: 0.6426 - Time: 46.80s\n",
      "[EP 45/100] - Reward: -122.0460 - Steps: 105 - Eps: 0.6362 - Time: 38.03s\n",
      "[EP 46/100] - Reward: -180.2843 - Steps: 85 - Eps: 0.6298 - Time: 31.33s\n",
      "[EP 47/100] - Reward: -123.3821 - Steps: 127 - Eps: 0.6235 - Time: 45.61s\n",
      "[EP 48/100] - Reward: 15.0303 - Steps: 125 - Eps: 0.6173 - Time: 48.58s\n",
      "[EP 49/100] - Reward: -106.9152 - Steps: 146 - Eps: 0.6111 - Time: 53.02s\n",
      "[EP 50/100] - Reward: 18.1834 - Steps: 74 - Eps: 0.6050 - Time: 27.16s\n",
      "[EP 51/100] - Reward: -62.7100 - Steps: 124 - Eps: 0.5990 - Time: 46.21s\n",
      "[EP 52/100] - Reward: -139.7260 - Steps: 107 - Eps: 0.5930 - Time: 38.36s\n",
      "[EP 53/100] - Reward: -80.6969 - Steps: 61 - Eps: 0.5870 - Time: 23.13s\n",
      "[EP 54/100] - Reward: -73.0502 - Steps: 123 - Eps: 0.5812 - Time: 43.77s\n",
      "[EP 55/100] - Reward: -138.5432 - Steps: 103 - Eps: 0.5754 - Time: 38.11s\n",
      "[EP 56/100] - Reward: -51.3379 - Steps: 69 - Eps: 0.5696 - Time: 25.26s\n",
      "[EP 57/100] - Reward: -66.8415 - Steps: 121 - Eps: 0.5639 - Time: 44.86s\n",
      "[EP 58/100] - Reward: -59.4953 - Steps: 79 - Eps: 0.5583 - Time: 28.45s\n",
      "[EP 59/100] - Reward: -33.1483 - Steps: 96 - Eps: 0.5527 - Time: 34.97s\n",
      "[EP 60/100] - Reward: -78.2987 - Steps: 134 - Eps: 0.5472 - Time: 48.92s\n",
      "[EP 61/100] - Reward: -196.0311 - Steps: 94 - Eps: 0.5417 - Time: 35.63s\n",
      "[EP 62/100] - Reward: -44.8739 - Steps: 123 - Eps: 0.5363 - Time: 44.67s\n",
      "[EP 63/100] - Reward: -86.0517 - Steps: 119 - Eps: 0.5309 - Time: 43.72s\n",
      "[EP 64/100] - Reward: 5.0854 - Steps: 191 - Eps: 0.5256 - Time: 69.44s\n",
      "[EP 65/100] - Reward: 6.5553 - Steps: 122 - Eps: 0.5203 - Time: 45.12s\n",
      "[EP 66/100] - Reward: -3.0405 - Steps: 89 - Eps: 0.5151 - Time: 33.10s\n",
      "[EP 67/100] - Reward: -2.9817 - Steps: 87 - Eps: 0.5100 - Time: 31.63s\n",
      "[EP 68/100] - Reward: -23.1633 - Steps: 88 - Eps: 0.5049 - Time: 32.90s\n",
      "[EP 69/100] - Reward: -51.4058 - Steps: 86 - Eps: 0.4998 - Time: 33.48s\n",
      "[EP 70/100] - Reward: -136.1607 - Steps: 96 - Eps: 0.4948 - Time: 39.14s\n",
      "[EP 71/100] - Reward: -15.5927 - Steps: 109 - Eps: 0.4899 - Time: 42.96s\n",
      "[EP 72/100] - Reward: -92.4813 - Steps: 103 - Eps: 0.4850 - Time: 40.56s\n",
      "[EP 73/100] - Reward: -17.3496 - Steps: 132 - Eps: 0.4801 - Time: 51.52s\n",
      "[EP 74/100] - Reward: -31.7687 - Steps: 117 - Eps: 0.4753 - Time: 49.12s\n",
      "[EP 75/100] - Reward: -47.6223 - Steps: 111 - Eps: 0.4706 - Time: 49.27s\n",
      "[EP 76/100] - Reward: -48.2711 - Steps: 110 - Eps: 0.4659 - Time: 53.17s\n",
      "[EP 77/100] - Reward: -59.3828 - Steps: 129 - Eps: 0.4612 - Time: 66.60s\n",
      "[EP 78/100] - Reward: -41.7922 - Steps: 109 - Eps: 0.4566 - Time: 53.37s\n",
      "[EP 79/100] - Reward: 2.5813 - Steps: 92 - Eps: 0.4520 - Time: 42.66s\n",
      "[EP 80/100] - Reward: -185.8301 - Steps: 172 - Eps: 0.4475 - Time: 78.63s\n",
      "[EP 81/100] - Reward: 18.7638 - Steps: 89 - Eps: 0.4430 - Time: 42.02s\n",
      "[EP 82/100] - Reward: -35.3160 - Steps: 115 - Eps: 0.4386 - Time: 54.43s\n",
      "[EP 83/100] - Reward: -5.4645 - Steps: 132 - Eps: 0.4342 - Time: 60.08s\n",
      "[EP 84/100] - Reward: -2.8235 - Steps: 162 - Eps: 0.4299 - Time: 75.83s\n",
      "[EP 85/100] - Reward: 16.5701 - Steps: 129 - Eps: 0.4256 - Time: 58.58s\n",
      "[EP 86/100] - Reward: -35.6417 - Steps: 102 - Eps: 0.4213 - Time: 47.17s\n",
      "[EP 87/100] - Reward: -11.6577 - Steps: 606 - Eps: 0.4171 - Time: 279.11s\n",
      "[EP 88/100] - Reward: -208.4444 - Steps: 132 - Eps: 0.4129 - Time: 62.51s\n",
      "[EP 89/100] - Reward: 13.6478 - Steps: 153 - Eps: 0.4088 - Time: 81.97s\n",
      "[EP 90/100] - Reward: -122.8374 - Steps: 122 - Eps: 0.4047 - Time: 56.47s\n",
      "[EP 91/100] - Reward: -129.5240 - Steps: 131 - Eps: 0.4007 - Time: 59.94s\n",
      "[EP 92/100] - Reward: -142.6931 - Steps: 149 - Eps: 0.3967 - Time: 70.04s\n",
      "[EP 93/100] - Reward: -62.6569 - Steps: 100 - Eps: 0.3927 - Time: 45.33s\n",
      "[EP 94/100] - Reward: -65.3908 - Steps: 199 - Eps: 0.3888 - Time: 93.06s\n",
      "[EP 95/100] - Reward: -24.9203 - Steps: 140 - Eps: 0.3849 - Time: 65.47s\n",
      "[EP 96/100] - Reward: 96.7228 - Steps: 1000 - Eps: 0.3810 - Time: 495.70s\n",
      "[EP 97/100] - Reward: -1.9119 - Steps: 132 - Eps: 0.3772 - Time: 58.72s\n",
      "[EP 98/100] - Reward: 14.0163 - Steps: 233 - Eps: 0.3735 - Time: 100.00s\n",
      "[EP 99/100] - Reward: 21.2120 - Steps: 140 - Eps: 0.3697 - Time: 60.56s\n",
      "[EP 100/100] - Reward: -101.4117 - Steps: 163 - Eps: 0.3660 - Time: 69.22s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>██▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>reward</td><td>▆▁▆▆▄▁▆▆▄▅▆▇▇▇▅▇▆▆▅▆▇▇▇▇▅██▇▆▇▇█▇███▆▇█▆</td></tr><tr><td>steps</td><td>▁▂▁▁▁▁▁▁▂▂▂▁▂▁▂▁▂▂▁▂▂▂▁▁▁▃▁▁▂▂▂▁▂▂█▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.36603</td></tr><tr><td>reward</td><td>-101.41168</td></tr><tr><td>steps</td><td>163</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-dragon-23</strong> at: <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/hsl2fiyo\" target=\"_blank\">https://wandb.ai/timothyckl/lunar-lander-rl/runs/hsl2fiyo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230124_143746-hsl2fiyo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = DDQL(env, learning_rate, discount_factor, exploration_rate)\n",
    "history = agent.train(episodes, max_steps, log_wandb=True, update=True, save_episodes=False, save_interval=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test agent\n",
    "history = agent.train(10, 1000, log_wandb=False, update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history\n",
    "# if not os.path.exists('./history'):\n",
    "#         os.mkdir('./history')\n",
    "\n",
    "# with open('./history/ddql_history.json', 'w') as file:\n",
    "#     json.dump(history, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Oct 19 2022, 22:38:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
