{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b560aa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T20:46:20.565657Z",
     "iopub.status.busy": "2023-01-27T20:46:20.564762Z",
     "iopub.status.idle": "2023-01-27T20:46:45.120736Z",
     "shell.execute_reply": "2023-01-27T20:46:45.119133Z"
    },
    "papermill": {
     "duration": 24.564119,
     "end_time": "2023-01-27T20:46:45.123341",
     "exception": false,
     "start_time": "2023-01-27T20:46:20.559222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\r\n",
      "  Downloading gymnasium-0.27.1-py3-none-any.whl (883 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.7/site-packages (0.14.0)\r\n",
      "Collecting jax-jumpy>=0.2.0\r\n",
      "  Downloading jax_jumpy-0.2.0-py3-none-any.whl (11 kB)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gymnasium) (2.1.0)\r\n",
      "Collecting typing-extensions>=4.3.0\r\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/conda/lib/python3.7/site-packages (from gymnasium) (4.13.0)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from gymnasium) (1.21.6)\r\n",
      "Collecting gymnasium-notices>=0.0.1\r\n",
      "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\r\n",
      "Collecting pygame==2.1.3.dev8\r\n",
      "  Downloading pygame-2.1.3.dev8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting swig==4.*\r\n",
      "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting box2d-py==2.3.5\r\n",
      "  Downloading box2d_py-2.3.5-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (2.13.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.8.0)\r\n",
      "Installing collected packages: swig, gymnasium-notices, box2d-py, typing-extensions, pygame, jax-jumpy, gymnasium\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.1.1\r\n",
      "    Uninstalling typing_extensions-4.1.1:\r\n",
      "      Successfully uninstalled typing_extensions-4.1.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\r\n",
      "thinc 8.0.17 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.4.0 which is incompatible.\r\n",
      "tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "tensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "spacy 3.3.2 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\r\n",
      "pandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\r\n",
      "flake8 5.0.4 requires importlib-metadata<4.3,>=1.1.0; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\r\n",
      "confection 0.0.3 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\r\n",
      "cmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 4.13.0 which is incompatible.\r\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\r\n",
      "aiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.44 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed box2d-py-2.3.5 gymnasium-0.27.1 gymnasium-notices-0.0.1 jax-jumpy-0.2.0 pygame-2.1.3.dev8 swig-4.1.1 typing-extensions-4.4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium gymnasium[box2d] tensorflow-addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60da2873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T20:46:45.134607Z",
     "iopub.status.busy": "2023-01-27T20:46:45.134297Z",
     "iopub.status.idle": "2023-01-27T20:46:53.421694Z",
     "shell.execute_reply": "2023-01-27T20:46:53.420705Z"
    },
    "papermill": {
     "duration": 8.295672,
     "end_time": "2023-01-27T20:46:53.424184",
     "exception": false,
     "start_time": "2023-01-27T20:46:45.128512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import wandb\n",
    "import numpy as np\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import gymnasium as gym\n",
    "\n",
    "from time import time\n",
    "from tensorflow_addons.layers import NoisyDense\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fe6cb0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-27T20:46:53.435568Z",
     "iopub.status.busy": "2023-01-27T20:46:53.434891Z",
     "iopub.status.idle": "2023-01-27T20:46:53.450070Z",
     "shell.execute_reply": "2023-01-27T20:46:53.448968Z"
    },
    "papermill": {
     "duration": 0.023245,
     "end_time": "2023-01-27T20:46:53.452483",
     "exception": false,
     "start_time": "2023-01-27T20:46:53.429238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EpisodeSaver:\n",
    "    def __init__(self, env, frames, algo, episode_number):\n",
    "        self.env = env\n",
    "        self.frames = frames\n",
    "        self.dir = f'./gifs/{algo}/'\n",
    "        self.episode_number = episode_number\n",
    "        self.fname = f'episode_{self.episode_number}.gif'\n",
    "\n",
    "        if not os.path.exists('./gifs'):\n",
    "            os.mkdir('./gifs')\n",
    "\n",
    "        if not os.path.exists(self.dir):\n",
    "            os.mkdir(self.dir)\n",
    "\n",
    "    def label_frames(self):\n",
    "        labeled_frames = []\n",
    "\n",
    "        for frame in self.frames:\n",
    "            img = Image.fromarray(frame)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            # draw on each frame\n",
    "            draw.text((10, 10), f'Episode: {self.episode_number}', fill=(255, 255, 255))\n",
    "            labeled_frames.append(np.array(img))\n",
    "\n",
    "        return labeled_frames\n",
    "\n",
    "    def save(self):\n",
    "        labeled_frames = self.label_frames()\n",
    "        imageio.mimsave(self.dir + self.fname, labeled_frames, fps=60)\n",
    "        \n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_length, state_size, action_size):\n",
    "        self.memory_counter = 0\n",
    "        self.max_length = max_length\n",
    "        self.state_memory = np.zeros((self.max_length, state_size))\n",
    "        self.new_state_memory = np.zeros((self.max_length, state_size))\n",
    "        self.action_memory = np.zeros((self.max_length, action_size), dtype=np.int8)\n",
    "        self.reward_memory = np.zeros(self.max_length)\n",
    "        self.done_memory = np.zeros(self.max_length, dtype=np.float32)\n",
    "\n",
    "    def append(self, state, action, reward, new_state, done):\n",
    "        idx = self.memory_counter % self.max_length\n",
    "\n",
    "        self.state_memory[idx] = state\n",
    "        actions = np.zeros(self.action_memory.shape[1])\n",
    "        actions[action] = 1.0\n",
    "        self.action_memory[idx] = actions\n",
    "        self.new_state_memory[idx] = new_state\n",
    "        self.reward_memory[idx] = reward\n",
    "        self.done_memory[idx] = 1 - done\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_memory = min(self.memory_counter, self.max_length)\n",
    "        sampled_batch = np.random.choice(max_memory, batch_size)\n",
    "        \n",
    "        states= self.state_memory[sampled_batch]\n",
    "        actions = self.action_memory[sampled_batch]\n",
    "        rewards= self.reward_memory[sampled_batch]\n",
    "        new_states = self.new_state_memory[sampled_batch]\n",
    "        dones = self.done_memory[sampled_batch]\n",
    "\n",
    "        return states, actions, rewards, new_states, dones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08948953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T20:46:53.462919Z",
     "iopub.status.busy": "2023-01-27T20:46:53.462635Z",
     "iopub.status.idle": "2023-01-27T20:46:53.485279Z",
     "shell.execute_reply": "2023-01-27T20:46:53.484392Z"
    },
    "papermill": {
     "duration": 0.030371,
     "end_time": "2023-01-27T20:46:53.487323",
     "exception": false,
     "start_time": "2023-01-27T20:46:53.456952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQL:\n",
    "    def __init__(self, env, alpha, gamma, epsilon, epsilon_decay=0.99, epsilon_min=0.01, batch_size=64):\n",
    "        self.env = env \n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.action_space = [i for i in range(self.action_size)]\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayBuffer(10_000, self.state_size, self.action_size)\n",
    "        self.qnet = self.create_qnet('qnet')\n",
    "\n",
    "    def create_qnet(self, name):\n",
    "        model = Sequential([\n",
    "            NoisyDense(units=256, activation='relu', input_shape=(self.state_size,)),\n",
    "            NoisyDense(units=256, activation='relu'),\n",
    "            NoisyDense(units=int(self.action_size), activation='linear')\n",
    "        ], name=name)\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.alpha))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append(state, action, reward, new_state, done)\n",
    "\n",
    "    def act(self, state):\n",
    "        '''\n",
    "        Epsilon-greedy policy is used to choose action.\n",
    "        This means that if we choose to exploit, we choose the action with the highest Q-value.\n",
    "        '''\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        rand = np.random.random()\n",
    "        \n",
    "        if rand < self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:\n",
    "            return np.argmax(self.qnet.predict(state, verbose=0))\n",
    "\n",
    "    def update(self):\n",
    "        '''\n",
    "        Q(S, A) ← Q(S, A) + α[R + γmax_a'Q(S', a) - Q(S, A)]\n",
    "\n",
    "        where \n",
    "        - Q(S, A) is being updated\n",
    "        - Q(s, a) is the current Q-value\n",
    "        - R + γmax_a'Q(S', a) is the target Q-value\n",
    "        '''\n",
    "        if self.memory.memory_counter > self.batch_size:\n",
    "            state, action, reward, new_state, done = self.memory.sample(self.batch_size)\n",
    "\n",
    "            action_values = np.array(self.action_space, dtype=np.int8)\n",
    "            action_idx = np.dot(action, action_values)\n",
    "\n",
    "            q_current = self.qnet.predict(state, verbose=0)\n",
    "            q_future = self.qnet.predict(new_state, verbose=0)\n",
    "            q_target = q_current.copy()\n",
    "\n",
    "            batch_idx = np.arange(self.batch_size, dtype=np.int32)\n",
    "            q_target[batch_idx, action_idx] = reward + self.gamma * np.max(q_future, axis=1) * done\n",
    "\n",
    "            self.qnet.fit(x=state, y=q_target, verbose=0)\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    def train(self, n_episodes, max_steps=1000, log_wandb=False, \n",
    "                update=True, save_episodes=False, save_interval=10):\n",
    "        '''\n",
    "        ---------------------------------------------------------\n",
    "        Deep Q-Learning with Experience Replay\n",
    "        ---------------------------------------------------------\n",
    "        Initialize replay memory D to capacity N\n",
    "        Initialize action-value function Q with random weights\n",
    "\n",
    "        for episode = 1, M do\n",
    "            Initialize sequence s1 = {x1} and preprocessed sequence φ1 = φ(s1)\n",
    "            for t = 1, T do\n",
    "                With probability ε select a random action at\n",
    "                otherwise select at = argmaxaQ(φ(st), a; θ)\n",
    "                Execute action at in emulator and observe reward rt and new state st+1\n",
    "                Set st+1 = st, at and preprocess φt+1 = φ(st+1)\n",
    "                Store transition (φt, at, rt, φt+1) in D\n",
    "                Sample random minibatch of transitions (φj , aj , rj , φj+1) from D\n",
    "                Set yj = rj if the episode ends at j + 1, otherwise yj = rj + γmaxaQ(φj+1, a; θ)\n",
    "                Perform a gradient descent step on (yj − Q(φj , aj ; θ))2 with respect to the network parameters θ\n",
    "            end for\n",
    "        end for\n",
    "        '''\n",
    "        history = {'reward': [], 'avg_reward_100': [], 'steps': []}\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            start_time = time()\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            frames = []\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                action = self.act(state)\n",
    "                new_state, reward, done, _, _ = self.env.step(action)\n",
    "                frames.append(self.env.render())\n",
    "\n",
    "                if update:\n",
    "                    self.remember(state, action, reward, new_state, done)\n",
    "                    self.update()\n",
    "\n",
    "                state = new_state\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "            if log_wandb:\n",
    "                wandb.log({\n",
    "                    'reward': episode_reward,\n",
    "                    'steps': episode_steps, \n",
    "                    'epsilon': self.epsilon\n",
    "                })\n",
    "\n",
    "            if save_episodes:\n",
    "                if (episode + 1) % save_interval == 0 or (episode == 0):\n",
    "                    s = EpisodeSaver(self.env, frames, 'DQL', episode + 1)\n",
    "                    s.save()\n",
    "\n",
    "            print(f'[EP {episode + 1}/{n_episodes}] - Reward: {episode_reward:.4f} - Steps: {episode_steps} - Eps: {self.epsilon:.4f} - Time: {time() - start_time:.2f}s')\n",
    "\n",
    "            history['reward'].append(episode_reward)\n",
    "            history['avg_reward_100'].append(np.mean(history['reward'][-100:]))\n",
    "            history['steps'].append(episode_steps)\n",
    "\n",
    "        self.env.close()\n",
    "        \n",
    "        if log_wandb:\n",
    "            wandb.finish()\n",
    "\n",
    "        self.save('dql.h5')\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def save(self, fname):\n",
    "        if not os.path.exists('./assets'):\n",
    "            os.mkdir('./assets')\n",
    "\n",
    "        self.qnet.save(f'./assets/{fname}')\n",
    "\n",
    "    def load(self, fname):\n",
    "        self.qnet = load_model(f'./assets/{fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0d9d63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T20:46:53.498184Z",
     "iopub.status.busy": "2023-01-27T20:46:53.497334Z",
     "iopub.status.idle": "2023-01-27T20:46:56.235353Z",
     "shell.execute_reply": "2023-01-27T20:46:56.234032Z"
    },
    "papermill": {
     "duration": 2.746282,
     "end_time": "2023-01-27T20:46:56.238180",
     "exception": false,
     "start_time": "2023-01-27T20:46:53.491898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login 5f71bc6f91cdaa551a70e88cf2522fcc1425d29b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d21c53d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T20:46:56.249985Z",
     "iopub.status.busy": "2023-01-27T20:46:56.249651Z",
     "iopub.status.idle": "2023-01-27T20:47:04.189757Z",
     "shell.execute_reply": "2023-01-27T20:47:04.188750Z"
    },
    "papermill": {
     "duration": 7.949142,
     "end_time": "2023-01-27T20:47:04.192492",
     "exception": false,
     "start_time": "2023-01-27T20:46:56.243350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimothyckl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20230127_204657-27aql4y9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/27aql4y9\" target=\"_blank\">glistening-rat-63</a></strong> to <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/27aql4y9?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f86f2e11850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', continuous=False, render_mode='rgb_array')\n",
    "learning_rate = 0.0001\n",
    "discount_factor = 0.79\n",
    "exploration_rate = 1.0\n",
    "episodes = 500\n",
    "max_steps = 1000\n",
    "\n",
    "wandb.init(project='lunar-lander-rl', entity='timothyckl', config={\n",
    "    'learning_rate': learning_rate,\n",
    "    'discount_factor': discount_factor,\n",
    "    'exploration_rate': exploration_rate,\n",
    "    'episodes': episodes,\n",
    "    'max_steps': max_steps\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d4bfd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T20:47:04.211066Z",
     "iopub.status.busy": "2023-01-27T20:47:04.210679Z",
     "iopub.status.idle": "2023-01-28T06:26:03.149636Z",
     "shell.execute_reply": "2023-01-28T06:26:03.148612Z"
    },
    "papermill": {
     "duration": 34738.950563,
     "end_time": "2023-01-28T06:26:03.151757",
     "exception": false,
     "start_time": "2023-01-27T20:47:04.201194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 1/500] - Reward: -199.2271 - Steps: 104 - Eps: 0.6690 - Time: 12.13s\n",
      "[EP 2/500] - Reward: -114.8536 - Steps: 65 - Eps: 0.3481 - Time: 12.75s\n",
      "[EP 3/500] - Reward: -210.0770 - Steps: 90 - Eps: 0.1409 - Time: 18.21s\n",
      "[EP 4/500] - Reward: -620.7632 - Steps: 81 - Eps: 0.0624 - Time: 12.49s\n",
      "[EP 5/500] - Reward: -113.1948 - Steps: 58 - Eps: 0.0348 - Time: 8.71s\n",
      "[EP 6/500] - Reward: -341.0526 - Steps: 82 - Eps: 0.0153 - Time: 12.14s\n",
      "[EP 7/500] - Reward: -99.6255 - Steps: 74 - Eps: 0.0100 - Time: 12.03s\n",
      "[EP 8/500] - Reward: -126.1074 - Steps: 91 - Eps: 0.0100 - Time: 13.79s\n",
      "[EP 9/500] - Reward: 5.9744 - Steps: 69 - Eps: 0.0100 - Time: 10.94s\n",
      "[EP 10/500] - Reward: -270.9009 - Steps: 98 - Eps: 0.0100 - Time: 15.40s\n",
      "[EP 11/500] - Reward: -231.1958 - Steps: 94 - Eps: 0.0100 - Time: 14.53s\n",
      "[EP 12/500] - Reward: -296.9996 - Steps: 88 - Eps: 0.0100 - Time: 13.19s\n",
      "[EP 13/500] - Reward: -148.8141 - Steps: 51 - Eps: 0.0100 - Time: 7.77s\n",
      "[EP 14/500] - Reward: -524.8676 - Steps: 94 - Eps: 0.0100 - Time: 14.55s\n",
      "[EP 15/500] - Reward: -164.1246 - Steps: 53 - Eps: 0.0100 - Time: 8.28s\n",
      "[EP 16/500] - Reward: -224.0410 - Steps: 88 - Eps: 0.0100 - Time: 13.19s\n",
      "[EP 17/500] - Reward: -150.6256 - Steps: 68 - Eps: 0.0100 - Time: 10.90s\n",
      "[EP 18/500] - Reward: -178.8287 - Steps: 76 - Eps: 0.0100 - Time: 11.75s\n",
      "[EP 19/500] - Reward: -76.1031 - Steps: 108 - Eps: 0.0100 - Time: 16.77s\n",
      "[EP 20/500] - Reward: -215.6467 - Steps: 129 - Eps: 0.0100 - Time: 19.75s\n",
      "[EP 21/500] - Reward: -422.9295 - Steps: 343 - Eps: 0.0100 - Time: 52.93s\n",
      "[EP 22/500] - Reward: -309.6926 - Steps: 179 - Eps: 0.0100 - Time: 27.40s\n",
      "[EP 23/500] - Reward: -262.4257 - Steps: 344 - Eps: 0.0100 - Time: 52.98s\n",
      "[EP 24/500] - Reward: -239.4399 - Steps: 557 - Eps: 0.0100 - Time: 86.00s\n",
      "[EP 25/500] - Reward: -249.4281 - Steps: 575 - Eps: 0.0100 - Time: 88.61s\n",
      "[EP 26/500] - Reward: -297.2643 - Steps: 176 - Eps: 0.0100 - Time: 27.08s\n",
      "[EP 27/500] - Reward: -349.6978 - Steps: 170 - Eps: 0.0100 - Time: 26.45s\n",
      "[EP 28/500] - Reward: -247.7173 - Steps: 661 - Eps: 0.0100 - Time: 101.46s\n",
      "[EP 29/500] - Reward: -193.6297 - Steps: 484 - Eps: 0.0100 - Time: 73.75s\n",
      "[EP 30/500] - Reward: -142.7549 - Steps: 268 - Eps: 0.0100 - Time: 40.79s\n",
      "[EP 31/500] - Reward: -172.4148 - Steps: 435 - Eps: 0.0100 - Time: 67.05s\n",
      "[EP 32/500] - Reward: -173.5143 - Steps: 335 - Eps: 0.0100 - Time: 51.04s\n",
      "[EP 33/500] - Reward: -136.7721 - Steps: 1000 - Eps: 0.0100 - Time: 153.55s\n",
      "[EP 34/500] - Reward: -148.6325 - Steps: 1000 - Eps: 0.0100 - Time: 153.30s\n",
      "[EP 35/500] - Reward: -158.0989 - Steps: 1000 - Eps: 0.0100 - Time: 153.80s\n",
      "[EP 36/500] - Reward: -220.1126 - Steps: 744 - Eps: 0.0100 - Time: 114.43s\n",
      "[EP 37/500] - Reward: -185.5896 - Steps: 607 - Eps: 0.0100 - Time: 93.08s\n",
      "[EP 38/500] - Reward: -252.7393 - Steps: 895 - Eps: 0.0100 - Time: 137.14s\n",
      "[EP 39/500] - Reward: -224.0429 - Steps: 594 - Eps: 0.0100 - Time: 90.78s\n",
      "[EP 40/500] - Reward: -178.9974 - Steps: 228 - Eps: 0.0100 - Time: 35.12s\n",
      "[EP 41/500] - Reward: -190.6267 - Steps: 633 - Eps: 0.0100 - Time: 96.51s\n",
      "[EP 42/500] - Reward: -85.2660 - Steps: 1000 - Eps: 0.0100 - Time: 152.24s\n",
      "[EP 43/500] - Reward: -113.5445 - Steps: 1000 - Eps: 0.0100 - Time: 155.25s\n",
      "[EP 44/500] - Reward: -135.0023 - Steps: 1000 - Eps: 0.0100 - Time: 157.28s\n",
      "[EP 45/500] - Reward: -161.3346 - Steps: 1000 - Eps: 0.0100 - Time: 162.98s\n",
      "[EP 46/500] - Reward: -145.5863 - Steps: 1000 - Eps: 0.0100 - Time: 154.93s\n",
      "[EP 47/500] - Reward: -127.0770 - Steps: 1000 - Eps: 0.0100 - Time: 155.78s\n",
      "[EP 48/500] - Reward: -114.4535 - Steps: 1000 - Eps: 0.0100 - Time: 162.42s\n",
      "[EP 49/500] - Reward: -54.9862 - Steps: 321 - Eps: 0.0100 - Time: 50.65s\n",
      "[EP 50/500] - Reward: -131.1752 - Steps: 238 - Eps: 0.0100 - Time: 36.09s\n",
      "[EP 51/500] - Reward: -123.9992 - Steps: 1000 - Eps: 0.0100 - Time: 153.45s\n",
      "[EP 52/500] - Reward: -110.5635 - Steps: 1000 - Eps: 0.0100 - Time: 163.35s\n",
      "[EP 53/500] - Reward: -115.9727 - Steps: 1000 - Eps: 0.0100 - Time: 178.37s\n",
      "[EP 54/500] - Reward: -83.3790 - Steps: 1000 - Eps: 0.0100 - Time: 177.05s\n",
      "[EP 55/500] - Reward: -134.8846 - Steps: 1000 - Eps: 0.0100 - Time: 177.79s\n",
      "[EP 56/500] - Reward: -126.1026 - Steps: 1000 - Eps: 0.0100 - Time: 176.58s\n",
      "[EP 57/500] - Reward: -81.4555 - Steps: 365 - Eps: 0.0100 - Time: 64.40s\n",
      "[EP 58/500] - Reward: -656.1827 - Steps: 905 - Eps: 0.0100 - Time: 159.95s\n",
      "[EP 59/500] - Reward: -184.8388 - Steps: 598 - Eps: 0.0100 - Time: 106.63s\n",
      "[EP 60/500] - Reward: -299.7841 - Steps: 779 - Eps: 0.0100 - Time: 140.35s\n",
      "[EP 61/500] - Reward: -160.2403 - Steps: 1000 - Eps: 0.0100 - Time: 180.52s\n",
      "[EP 62/500] - Reward: -253.3988 - Steps: 290 - Eps: 0.0100 - Time: 52.26s\n",
      "[EP 63/500] - Reward: -85.1359 - Steps: 1000 - Eps: 0.0100 - Time: 164.59s\n",
      "[EP 64/500] - Reward: -153.8732 - Steps: 1000 - Eps: 0.0100 - Time: 163.77s\n",
      "[EP 65/500] - Reward: -175.9332 - Steps: 1000 - Eps: 0.0100 - Time: 151.81s\n",
      "[EP 66/500] - Reward: -282.3427 - Steps: 127 - Eps: 0.0100 - Time: 19.83s\n",
      "[EP 67/500] - Reward: -121.1373 - Steps: 1000 - Eps: 0.0100 - Time: 163.09s\n",
      "[EP 68/500] - Reward: -466.1715 - Steps: 86 - Eps: 0.0100 - Time: 14.49s\n",
      "[EP 69/500] - Reward: -151.3152 - Steps: 1000 - Eps: 0.0100 - Time: 176.94s\n",
      "[EP 70/500] - Reward: -262.5334 - Steps: 147 - Eps: 0.0100 - Time: 25.79s\n",
      "[EP 71/500] - Reward: -155.5216 - Steps: 1000 - Eps: 0.0100 - Time: 177.94s\n",
      "[EP 72/500] - Reward: -214.0193 - Steps: 302 - Eps: 0.0100 - Time: 54.27s\n",
      "[EP 73/500] - Reward: -235.8503 - Steps: 149 - Eps: 0.0100 - Time: 26.88s\n",
      "[EP 74/500] - Reward: -46.6431 - Steps: 203 - Eps: 0.0100 - Time: 36.17s\n",
      "[EP 75/500] - Reward: -70.8893 - Steps: 1000 - Eps: 0.0100 - Time: 178.33s\n",
      "[EP 76/500] - Reward: -112.3354 - Steps: 1000 - Eps: 0.0100 - Time: 179.84s\n",
      "[EP 77/500] - Reward: -41.9560 - Steps: 1000 - Eps: 0.0100 - Time: 181.03s\n",
      "[EP 78/500] - Reward: -45.1496 - Steps: 1000 - Eps: 0.0100 - Time: 180.83s\n",
      "[EP 79/500] - Reward: -126.4020 - Steps: 1000 - Eps: 0.0100 - Time: 180.64s\n",
      "[EP 80/500] - Reward: -111.3394 - Steps: 1000 - Eps: 0.0100 - Time: 178.86s\n",
      "[EP 81/500] - Reward: -73.0386 - Steps: 1000 - Eps: 0.0100 - Time: 176.75s\n",
      "[EP 82/500] - Reward: -50.2389 - Steps: 1000 - Eps: 0.0100 - Time: 176.33s\n",
      "[EP 83/500] - Reward: -4.4251 - Steps: 90 - Eps: 0.0100 - Time: 15.03s\n",
      "[EP 84/500] - Reward: -85.3360 - Steps: 1000 - Eps: 0.0100 - Time: 174.54s\n",
      "[EP 85/500] - Reward: -82.9680 - Steps: 1000 - Eps: 0.0100 - Time: 174.27s\n",
      "[EP 86/500] - Reward: -93.4286 - Steps: 1000 - Eps: 0.0100 - Time: 168.72s\n",
      "[EP 87/500] - Reward: -111.5637 - Steps: 66 - Eps: 0.0100 - Time: 10.84s\n",
      "[EP 88/500] - Reward: -16.2711 - Steps: 138 - Eps: 0.0100 - Time: 22.39s\n",
      "[EP 89/500] - Reward: 8.0117 - Steps: 67 - Eps: 0.0100 - Time: 11.37s\n",
      "[EP 90/500] - Reward: -88.0970 - Steps: 1000 - Eps: 0.0100 - Time: 164.39s\n",
      "[EP 91/500] - Reward: -97.2490 - Steps: 1000 - Eps: 0.0100 - Time: 158.33s\n",
      "[EP 92/500] - Reward: -74.9682 - Steps: 89 - Eps: 0.0100 - Time: 14.04s\n",
      "[EP 93/500] - Reward: -81.9848 - Steps: 65 - Eps: 0.0100 - Time: 9.82s\n",
      "[EP 94/500] - Reward: -144.3619 - Steps: 95 - Eps: 0.0100 - Time: 15.79s\n",
      "[EP 95/500] - Reward: -181.7308 - Steps: 99 - Eps: 0.0100 - Time: 15.38s\n",
      "[EP 96/500] - Reward: -486.3772 - Steps: 135 - Eps: 0.0100 - Time: 21.02s\n",
      "[EP 97/500] - Reward: -130.1096 - Steps: 134 - Eps: 0.0100 - Time: 21.05s\n",
      "[EP 98/500] - Reward: 40.2514 - Steps: 80 - Eps: 0.0100 - Time: 12.60s\n",
      "[EP 99/500] - Reward: -221.5801 - Steps: 115 - Eps: 0.0100 - Time: 17.68s\n",
      "[EP 100/500] - Reward: -81.6630 - Steps: 1000 - Eps: 0.0100 - Time: 183.07s\n",
      "[EP 101/500] - Reward: -264.5796 - Steps: 537 - Eps: 0.0100 - Time: 84.81s\n",
      "[EP 102/500] - Reward: -60.0649 - Steps: 1000 - Eps: 0.0100 - Time: 159.56s\n",
      "[EP 103/500] - Reward: -239.8549 - Steps: 115 - Eps: 0.0100 - Time: 18.49s\n",
      "[EP 104/500] - Reward: -235.0393 - Steps: 971 - Eps: 0.0100 - Time: 154.76s\n",
      "[EP 105/500] - Reward: -75.1654 - Steps: 205 - Eps: 0.0100 - Time: 32.54s\n",
      "[EP 106/500] - Reward: -104.0374 - Steps: 1000 - Eps: 0.0100 - Time: 159.74s\n",
      "[EP 107/500] - Reward: -353.7914 - Steps: 272 - Eps: 0.0100 - Time: 43.64s\n",
      "[EP 108/500] - Reward: -31.1150 - Steps: 1000 - Eps: 0.0100 - Time: 159.84s\n",
      "[EP 109/500] - Reward: -647.5369 - Steps: 84 - Eps: 0.0100 - Time: 13.11s\n",
      "[EP 110/500] - Reward: -41.9706 - Steps: 182 - Eps: 0.0100 - Time: 28.79s\n",
      "[EP 111/500] - Reward: -46.4265 - Steps: 435 - Eps: 0.0100 - Time: 68.76s\n",
      "[EP 112/500] - Reward: -7.5219 - Steps: 1000 - Eps: 0.0100 - Time: 158.02s\n",
      "[EP 113/500] - Reward: -12.1584 - Steps: 178 - Eps: 0.0100 - Time: 28.58s\n",
      "[EP 114/500] - Reward: -92.3187 - Steps: 190 - Eps: 0.0100 - Time: 30.26s\n",
      "[EP 115/500] - Reward: -42.8235 - Steps: 1000 - Eps: 0.0100 - Time: 157.99s\n",
      "[EP 116/500] - Reward: -31.8024 - Steps: 115 - Eps: 0.0100 - Time: 18.65s\n",
      "[EP 117/500] - Reward: -290.2158 - Steps: 334 - Eps: 0.0100 - Time: 52.53s\n",
      "[EP 118/500] - Reward: -20.0993 - Steps: 92 - Eps: 0.0100 - Time: 14.59s\n",
      "[EP 119/500] - Reward: -4.7375 - Steps: 1000 - Eps: 0.0100 - Time: 157.79s\n",
      "[EP 120/500] - Reward: -41.3300 - Steps: 147 - Eps: 0.0100 - Time: 22.85s\n",
      "[EP 121/500] - Reward: -251.2656 - Steps: 868 - Eps: 0.0100 - Time: 136.03s\n",
      "[EP 122/500] - Reward: -171.6653 - Steps: 190 - Eps: 0.0100 - Time: 30.26s\n",
      "[EP 123/500] - Reward: -213.4231 - Steps: 76 - Eps: 0.0100 - Time: 11.54s\n",
      "[EP 124/500] - Reward: -44.3716 - Steps: 99 - Eps: 0.0100 - Time: 15.76s\n",
      "[EP 125/500] - Reward: -250.3822 - Steps: 66 - Eps: 0.0100 - Time: 10.77s\n",
      "[EP 126/500] - Reward: -216.8441 - Steps: 112 - Eps: 0.0100 - Time: 17.96s\n",
      "[EP 127/500] - Reward: -132.3576 - Steps: 214 - Eps: 0.0100 - Time: 33.50s\n",
      "[EP 128/500] - Reward: -65.7876 - Steps: 277 - Eps: 0.0100 - Time: 43.58s\n",
      "[EP 129/500] - Reward: -207.0407 - Steps: 831 - Eps: 0.0100 - Time: 129.84s\n",
      "[EP 130/500] - Reward: -216.8331 - Steps: 108 - Eps: 0.0100 - Time: 16.80s\n",
      "[EP 131/500] - Reward: -393.3193 - Steps: 98 - Eps: 0.0100 - Time: 14.82s\n",
      "[EP 132/500] - Reward: -350.1617 - Steps: 114 - Eps: 0.0100 - Time: 18.44s\n",
      "[EP 133/500] - Reward: -414.4922 - Steps: 79 - Eps: 0.0100 - Time: 12.03s\n",
      "[EP 134/500] - Reward: -338.0345 - Steps: 134 - Eps: 0.0100 - Time: 21.12s\n",
      "[EP 135/500] - Reward: -362.0050 - Steps: 165 - Eps: 0.0100 - Time: 26.20s\n",
      "[EP 136/500] - Reward: -8.2319 - Steps: 1000 - Eps: 0.0100 - Time: 156.63s\n",
      "[EP 137/500] - Reward: -104.1351 - Steps: 199 - Eps: 0.0100 - Time: 31.60s\n",
      "[EP 138/500] - Reward: -38.2307 - Steps: 1000 - Eps: 0.0100 - Time: 156.02s\n",
      "[EP 139/500] - Reward: -16.6624 - Steps: 1000 - Eps: 0.0100 - Time: 157.11s\n",
      "[EP 140/500] - Reward: 22.9679 - Steps: 1000 - Eps: 0.0100 - Time: 157.54s\n",
      "[EP 141/500] - Reward: -35.5186 - Steps: 1000 - Eps: 0.0100 - Time: 157.08s\n",
      "[EP 142/500] - Reward: 20.4692 - Steps: 1000 - Eps: 0.0100 - Time: 155.50s\n",
      "[EP 143/500] - Reward: -65.9692 - Steps: 1000 - Eps: 0.0100 - Time: 159.06s\n",
      "[EP 144/500] - Reward: -40.3252 - Steps: 1000 - Eps: 0.0100 - Time: 154.62s\n",
      "[EP 145/500] - Reward: -48.0843 - Steps: 242 - Eps: 0.0100 - Time: 36.93s\n",
      "[EP 146/500] - Reward: 24.8959 - Steps: 1000 - Eps: 0.0100 - Time: 154.28s\n",
      "[EP 147/500] - Reward: -142.6902 - Steps: 204 - Eps: 0.0100 - Time: 31.16s\n",
      "[EP 148/500] - Reward: 31.2394 - Steps: 1000 - Eps: 0.0100 - Time: 153.28s\n",
      "[EP 149/500] - Reward: -13.8270 - Steps: 1000 - Eps: 0.0100 - Time: 152.87s\n",
      "[EP 150/500] - Reward: -25.6387 - Steps: 1000 - Eps: 0.0100 - Time: 153.36s\n",
      "[EP 151/500] - Reward: -105.4425 - Steps: 117 - Eps: 0.0100 - Time: 17.28s\n",
      "[EP 152/500] - Reward: 4.0048 - Steps: 1000 - Eps: 0.0100 - Time: 153.26s\n",
      "[EP 153/500] - Reward: -64.9218 - Steps: 97 - Eps: 0.0100 - Time: 15.05s\n",
      "[EP 154/500] - Reward: 168.1540 - Steps: 618 - Eps: 0.0100 - Time: 95.10s\n",
      "[EP 155/500] - Reward: -38.9447 - Steps: 1000 - Eps: 0.0100 - Time: 154.61s\n",
      "[EP 156/500] - Reward: -10.9653 - Steps: 1000 - Eps: 0.0100 - Time: 154.53s\n",
      "[EP 157/500] - Reward: -50.2263 - Steps: 1000 - Eps: 0.0100 - Time: 161.98s\n",
      "[EP 158/500] - Reward: 33.2096 - Steps: 1000 - Eps: 0.0100 - Time: 168.81s\n",
      "[EP 159/500] - Reward: 19.7485 - Steps: 1000 - Eps: 0.0100 - Time: 192.74s\n",
      "[EP 160/500] - Reward: 192.9859 - Steps: 681 - Eps: 0.0100 - Time: 132.47s\n",
      "[EP 161/500] - Reward: -214.0179 - Steps: 80 - Eps: 0.0100 - Time: 15.20s\n",
      "[EP 162/500] - Reward: 64.5473 - Steps: 1000 - Eps: 0.0100 - Time: 194.41s\n",
      "[EP 163/500] - Reward: -100.0928 - Steps: 96 - Eps: 0.0100 - Time: 18.75s\n",
      "[EP 164/500] - Reward: -62.8414 - Steps: 134 - Eps: 0.0100 - Time: 25.82s\n",
      "[EP 165/500] - Reward: 142.5142 - Steps: 873 - Eps: 0.0100 - Time: 168.73s\n",
      "[EP 166/500] - Reward: -21.0872 - Steps: 376 - Eps: 0.0100 - Time: 73.83s\n",
      "[EP 167/500] - Reward: -78.8734 - Steps: 772 - Eps: 0.0100 - Time: 149.73s\n",
      "[EP 168/500] - Reward: 172.3652 - Steps: 451 - Eps: 0.0100 - Time: 90.23s\n",
      "[EP 169/500] - Reward: -485.8302 - Steps: 378 - Eps: 0.0100 - Time: 75.03s\n",
      "[EP 170/500] - Reward: -312.2413 - Steps: 949 - Eps: 0.0100 - Time: 167.66s\n",
      "[EP 171/500] - Reward: -38.5852 - Steps: 1000 - Eps: 0.0100 - Time: 165.75s\n",
      "[EP 172/500] - Reward: -52.6398 - Steps: 208 - Eps: 0.0100 - Time: 34.30s\n",
      "[EP 173/500] - Reward: 190.2721 - Steps: 639 - Eps: 0.0100 - Time: 105.92s\n",
      "[EP 174/500] - Reward: 191.2983 - Steps: 583 - Eps: 0.0100 - Time: 95.36s\n",
      "[EP 175/500] - Reward: -68.2135 - Steps: 1000 - Eps: 0.0100 - Time: 161.86s\n",
      "[EP 176/500] - Reward: -69.8560 - Steps: 451 - Eps: 0.0100 - Time: 72.57s\n",
      "[EP 177/500] - Reward: 175.0731 - Steps: 511 - Eps: 0.0100 - Time: 81.47s\n",
      "[EP 178/500] - Reward: -72.2569 - Steps: 277 - Eps: 0.0100 - Time: 43.70s\n",
      "[EP 179/500] - Reward: 143.5822 - Steps: 953 - Eps: 0.0100 - Time: 159.84s\n",
      "[EP 180/500] - Reward: -20.5031 - Steps: 1000 - Eps: 0.0100 - Time: 155.40s\n",
      "[EP 181/500] - Reward: -6.2982 - Steps: 1000 - Eps: 0.0100 - Time: 161.27s\n",
      "[EP 182/500] - Reward: 236.6313 - Steps: 442 - Eps: 0.0100 - Time: 75.94s\n",
      "[EP 183/500] - Reward: 231.2094 - Steps: 274 - Eps: 0.0100 - Time: 49.52s\n",
      "[EP 184/500] - Reward: 222.9216 - Steps: 371 - Eps: 0.0100 - Time: 68.77s\n",
      "[EP 185/500] - Reward: -79.0280 - Steps: 387 - Eps: 0.0100 - Time: 70.60s\n",
      "[EP 186/500] - Reward: 243.4821 - Steps: 232 - Eps: 0.0100 - Time: 42.64s\n",
      "[EP 187/500] - Reward: 220.1143 - Steps: 578 - Eps: 0.0100 - Time: 105.66s\n",
      "[EP 188/500] - Reward: 247.7347 - Steps: 317 - Eps: 0.0100 - Time: 57.71s\n",
      "[EP 189/500] - Reward: -41.5311 - Steps: 188 - Eps: 0.0100 - Time: 34.65s\n",
      "[EP 190/500] - Reward: 243.5651 - Steps: 249 - Eps: 0.0100 - Time: 46.17s\n",
      "[EP 191/500] - Reward: -89.5980 - Steps: 88 - Eps: 0.0100 - Time: 17.07s\n",
      "[EP 192/500] - Reward: -81.4776 - Steps: 161 - Eps: 0.0100 - Time: 29.61s\n",
      "[EP 193/500] - Reward: -121.1540 - Steps: 112 - Eps: 0.0100 - Time: 20.54s\n",
      "[EP 194/500] - Reward: 254.8817 - Steps: 520 - Eps: 0.0100 - Time: 94.67s\n",
      "[EP 195/500] - Reward: 60.4907 - Steps: 1000 - Eps: 0.0100 - Time: 182.77s\n",
      "[EP 196/500] - Reward: 212.0024 - Steps: 296 - Eps: 0.0100 - Time: 53.96s\n",
      "[EP 197/500] - Reward: 259.1464 - Steps: 513 - Eps: 0.0100 - Time: 93.21s\n",
      "[EP 198/500] - Reward: -42.0787 - Steps: 116 - Eps: 0.0100 - Time: 20.78s\n",
      "[EP 199/500] - Reward: 278.8852 - Steps: 441 - Eps: 0.0100 - Time: 80.47s\n",
      "[EP 200/500] - Reward: 170.5273 - Steps: 593 - Eps: 0.0100 - Time: 123.99s\n",
      "[EP 201/500] - Reward: 190.9328 - Steps: 744 - Eps: 0.0100 - Time: 135.82s\n",
      "[EP 202/500] - Reward: 240.6035 - Steps: 383 - Eps: 0.0100 - Time: 69.87s\n",
      "[EP 203/500] - Reward: 227.9653 - Steps: 746 - Eps: 0.0100 - Time: 134.22s\n",
      "[EP 204/500] - Reward: 250.4453 - Steps: 193 - Eps: 0.0100 - Time: 34.95s\n",
      "[EP 205/500] - Reward: 229.1504 - Steps: 434 - Eps: 0.0100 - Time: 78.21s\n",
      "[EP 206/500] - Reward: 226.6370 - Steps: 724 - Eps: 0.0100 - Time: 130.15s\n",
      "[EP 207/500] - Reward: 7.7259 - Steps: 120 - Eps: 0.0100 - Time: 21.33s\n",
      "[EP 208/500] - Reward: 215.0391 - Steps: 297 - Eps: 0.0100 - Time: 53.06s\n",
      "[EP 209/500] - Reward: 155.2137 - Steps: 387 - Eps: 0.0100 - Time: 70.21s\n",
      "[EP 210/500] - Reward: 117.4257 - Steps: 1000 - Eps: 0.0100 - Time: 176.97s\n",
      "[EP 211/500] - Reward: -33.1615 - Steps: 376 - Eps: 0.0100 - Time: 65.21s\n",
      "[EP 212/500] - Reward: -72.7939 - Steps: 378 - Eps: 0.0100 - Time: 66.60s\n",
      "[EP 213/500] - Reward: -193.6905 - Steps: 788 - Eps: 0.0100 - Time: 134.38s\n",
      "[EP 214/500] - Reward: -96.9119 - Steps: 221 - Eps: 0.0100 - Time: 37.85s\n",
      "[EP 215/500] - Reward: 244.5531 - Steps: 526 - Eps: 0.0100 - Time: 88.64s\n",
      "[EP 216/500] - Reward: -47.3761 - Steps: 1000 - Eps: 0.0100 - Time: 165.30s\n",
      "[EP 217/500] - Reward: -65.4347 - Steps: 1000 - Eps: 0.0100 - Time: 164.72s\n",
      "[EP 218/500] - Reward: 260.6833 - Steps: 352 - Eps: 0.0100 - Time: 55.03s\n",
      "[EP 219/500] - Reward: 159.1901 - Steps: 888 - Eps: 0.0100 - Time: 140.86s\n",
      "[EP 220/500] - Reward: 95.9168 - Steps: 1000 - Eps: 0.0100 - Time: 158.56s\n",
      "[EP 221/500] - Reward: 111.0349 - Steps: 1000 - Eps: 0.0100 - Time: 157.74s\n",
      "[EP 222/500] - Reward: 236.6545 - Steps: 613 - Eps: 0.0100 - Time: 97.17s\n",
      "[EP 223/500] - Reward: 224.7696 - Steps: 653 - Eps: 0.0100 - Time: 103.01s\n",
      "[EP 224/500] - Reward: -522.7279 - Steps: 143 - Eps: 0.0100 - Time: 22.71s\n",
      "[EP 225/500] - Reward: 258.8114 - Steps: 269 - Eps: 0.0100 - Time: 42.89s\n",
      "[EP 226/500] - Reward: 300.3331 - Steps: 235 - Eps: 0.0100 - Time: 37.08s\n",
      "[EP 227/500] - Reward: 307.9605 - Steps: 206 - Eps: 0.0100 - Time: 31.71s\n",
      "[EP 228/500] - Reward: 284.4885 - Steps: 351 - Eps: 0.0100 - Time: 55.59s\n",
      "[EP 229/500] - Reward: 262.0976 - Steps: 292 - Eps: 0.0100 - Time: 46.31s\n",
      "[EP 230/500] - Reward: 279.7113 - Steps: 315 - Eps: 0.0100 - Time: 50.02s\n",
      "[EP 231/500] - Reward: 22.3393 - Steps: 106 - Eps: 0.0100 - Time: 16.44s\n",
      "[EP 232/500] - Reward: 218.6803 - Steps: 320 - Eps: 0.0100 - Time: 50.35s\n",
      "[EP 233/500] - Reward: 237.2272 - Steps: 421 - Eps: 0.0100 - Time: 67.53s\n",
      "[EP 234/500] - Reward: 236.2545 - Steps: 377 - Eps: 0.0100 - Time: 60.71s\n",
      "[EP 235/500] - Reward: 277.3660 - Steps: 220 - Eps: 0.0100 - Time: 34.96s\n",
      "[EP 236/500] - Reward: 283.9586 - Steps: 298 - Eps: 0.0100 - Time: 47.32s\n",
      "[EP 237/500] - Reward: 19.5026 - Steps: 114 - Eps: 0.0100 - Time: 17.57s\n",
      "[EP 238/500] - Reward: 252.9241 - Steps: 338 - Eps: 0.0100 - Time: 53.86s\n",
      "[EP 239/500] - Reward: 122.3422 - Steps: 1000 - Eps: 0.0100 - Time: 158.71s\n",
      "[EP 240/500] - Reward: -41.0000 - Steps: 69 - Eps: 0.0100 - Time: 10.79s\n",
      "[EP 241/500] - Reward: 27.3107 - Steps: 118 - Eps: 0.0100 - Time: 18.38s\n",
      "[EP 242/500] - Reward: 15.2371 - Steps: 107 - Eps: 0.0100 - Time: 17.25s\n",
      "[EP 243/500] - Reward: -45.2998 - Steps: 89 - Eps: 0.0100 - Time: 13.88s\n",
      "[EP 244/500] - Reward: -50.7736 - Steps: 77 - Eps: 0.0100 - Time: 11.78s\n",
      "[EP 245/500] - Reward: 5.7575 - Steps: 96 - Eps: 0.0100 - Time: 15.91s\n",
      "[EP 246/500] - Reward: 221.1392 - Steps: 267 - Eps: 0.0100 - Time: 42.07s\n",
      "[EP 247/500] - Reward: -22.8427 - Steps: 195 - Eps: 0.0100 - Time: 30.31s\n",
      "[EP 248/500] - Reward: -111.7939 - Steps: 83 - Eps: 0.0100 - Time: 13.03s\n",
      "[EP 249/500] - Reward: 232.5758 - Steps: 201 - Eps: 0.0100 - Time: 31.59s\n",
      "[EP 250/500] - Reward: -81.7527 - Steps: 78 - Eps: 0.0100 - Time: 12.79s\n",
      "[EP 251/500] - Reward: -63.5759 - Steps: 76 - Eps: 0.0100 - Time: 11.74s\n",
      "[EP 252/500] - Reward: 26.5588 - Steps: 1000 - Eps: 0.0100 - Time: 158.34s\n",
      "[EP 253/500] - Reward: 261.8581 - Steps: 855 - Eps: 0.0100 - Time: 135.35s\n",
      "[EP 254/500] - Reward: 243.8753 - Steps: 413 - Eps: 0.0100 - Time: 65.64s\n",
      "[EP 255/500] - Reward: 171.6844 - Steps: 528 - Eps: 0.0100 - Time: 83.40s\n",
      "[EP 256/500] - Reward: 255.2426 - Steps: 289 - Eps: 0.0100 - Time: 45.66s\n",
      "[EP 257/500] - Reward: 224.6608 - Steps: 821 - Eps: 0.0100 - Time: 130.49s\n",
      "[EP 258/500] - Reward: 215.6458 - Steps: 432 - Eps: 0.0100 - Time: 69.78s\n",
      "[EP 259/500] - Reward: 101.4116 - Steps: 1000 - Eps: 0.0100 - Time: 164.57s\n",
      "[EP 260/500] - Reward: -152.6969 - Steps: 290 - Eps: 0.0100 - Time: 47.76s\n",
      "[EP 261/500] - Reward: 218.3670 - Steps: 299 - Eps: 0.0100 - Time: 46.90s\n",
      "[EP 262/500] - Reward: 202.2069 - Steps: 306 - Eps: 0.0100 - Time: 47.63s\n",
      "[EP 263/500] - Reward: 228.7285 - Steps: 257 - Eps: 0.0100 - Time: 40.60s\n",
      "[EP 264/500] - Reward: 208.1451 - Steps: 261 - Eps: 0.0100 - Time: 41.23s\n",
      "[EP 265/500] - Reward: 54.1800 - Steps: 172 - Eps: 0.0100 - Time: 27.90s\n",
      "[EP 266/500] - Reward: 291.5886 - Steps: 330 - Eps: 0.0100 - Time: 51.56s\n",
      "[EP 267/500] - Reward: 279.7498 - Steps: 404 - Eps: 0.0100 - Time: 63.21s\n",
      "[EP 268/500] - Reward: 256.2435 - Steps: 195 - Eps: 0.0100 - Time: 30.91s\n",
      "[EP 269/500] - Reward: 251.6348 - Steps: 223 - Eps: 0.0100 - Time: 34.60s\n",
      "[EP 270/500] - Reward: 266.4127 - Steps: 318 - Eps: 0.0100 - Time: 49.76s\n",
      "[EP 271/500] - Reward: 224.7277 - Steps: 293 - Eps: 0.0100 - Time: 45.91s\n",
      "[EP 272/500] - Reward: 248.4883 - Steps: 378 - Eps: 0.0100 - Time: 59.18s\n",
      "[EP 273/500] - Reward: 253.7532 - Steps: 265 - Eps: 0.0100 - Time: 41.55s\n",
      "[EP 274/500] - Reward: 270.9070 - Steps: 275 - Eps: 0.0100 - Time: 42.79s\n",
      "[EP 275/500] - Reward: 285.5953 - Steps: 452 - Eps: 0.0100 - Time: 70.87s\n",
      "[EP 276/500] - Reward: 260.1330 - Steps: 189 - Eps: 0.0100 - Time: 29.12s\n",
      "[EP 277/500] - Reward: 246.0943 - Steps: 343 - Eps: 0.0100 - Time: 54.29s\n",
      "[EP 278/500] - Reward: 260.2457 - Steps: 408 - Eps: 0.0100 - Time: 63.90s\n",
      "[EP 279/500] - Reward: 278.7048 - Steps: 606 - Eps: 0.0100 - Time: 94.37s\n",
      "[EP 280/500] - Reward: 260.8237 - Steps: 362 - Eps: 0.0100 - Time: 57.24s\n",
      "[EP 281/500] - Reward: 253.9087 - Steps: 370 - Eps: 0.0100 - Time: 58.08s\n",
      "[EP 282/500] - Reward: 268.1346 - Steps: 891 - Eps: 0.0100 - Time: 139.20s\n",
      "[EP 283/500] - Reward: 238.2648 - Steps: 620 - Eps: 0.0100 - Time: 96.95s\n",
      "[EP 284/500] - Reward: 153.7136 - Steps: 1000 - Eps: 0.0100 - Time: 157.52s\n",
      "[EP 285/500] - Reward: 199.8316 - Steps: 224 - Eps: 0.0100 - Time: 35.47s\n",
      "[EP 286/500] - Reward: 284.8614 - Steps: 231 - Eps: 0.0100 - Time: 36.13s\n",
      "[EP 287/500] - Reward: 265.4154 - Steps: 217 - Eps: 0.0100 - Time: 34.29s\n",
      "[EP 288/500] - Reward: 40.2527 - Steps: 150 - Eps: 0.0100 - Time: 23.38s\n",
      "[EP 289/500] - Reward: 281.7858 - Steps: 383 - Eps: 0.0100 - Time: 59.98s\n",
      "[EP 290/500] - Reward: 286.9023 - Steps: 251 - Eps: 0.0100 - Time: 39.48s\n",
      "[EP 291/500] - Reward: 257.7016 - Steps: 258 - Eps: 0.0100 - Time: 40.96s\n",
      "[EP 292/500] - Reward: 250.2030 - Steps: 364 - Eps: 0.0100 - Time: 56.41s\n",
      "[EP 293/500] - Reward: 270.5529 - Steps: 236 - Eps: 0.0100 - Time: 37.64s\n",
      "[EP 294/500] - Reward: 287.5795 - Steps: 315 - Eps: 0.0100 - Time: 49.42s\n",
      "[EP 295/500] - Reward: 244.6623 - Steps: 224 - Eps: 0.0100 - Time: 35.34s\n",
      "[EP 296/500] - Reward: 24.7795 - Steps: 223 - Eps: 0.0100 - Time: 35.33s\n",
      "[EP 297/500] - Reward: 282.7603 - Steps: 373 - Eps: 0.0100 - Time: 57.90s\n",
      "[EP 298/500] - Reward: 250.8112 - Steps: 223 - Eps: 0.0100 - Time: 34.09s\n",
      "[EP 299/500] - Reward: 85.7778 - Steps: 173 - Eps: 0.0100 - Time: 27.28s\n",
      "[EP 300/500] - Reward: 227.3986 - Steps: 635 - Eps: 0.0100 - Time: 115.66s\n",
      "[EP 301/500] - Reward: 249.3470 - Steps: 217 - Eps: 0.0100 - Time: 33.59s\n",
      "[EP 302/500] - Reward: 259.8746 - Steps: 296 - Eps: 0.0100 - Time: 45.43s\n",
      "[EP 303/500] - Reward: 268.4120 - Steps: 249 - Eps: 0.0100 - Time: 38.79s\n",
      "[EP 304/500] - Reward: 315.1501 - Steps: 210 - Eps: 0.0100 - Time: 33.37s\n",
      "[EP 305/500] - Reward: 268.2434 - Steps: 220 - Eps: 0.0100 - Time: 33.98s\n",
      "[EP 306/500] - Reward: 244.3250 - Steps: 490 - Eps: 0.0100 - Time: 75.60s\n",
      "[EP 307/500] - Reward: 260.5314 - Steps: 205 - Eps: 0.0100 - Time: 31.35s\n",
      "[EP 308/500] - Reward: 267.7507 - Steps: 228 - Eps: 0.0100 - Time: 35.72s\n",
      "[EP 309/500] - Reward: 268.8600 - Steps: 245 - Eps: 0.0100 - Time: 37.62s\n",
      "[EP 310/500] - Reward: 264.2733 - Steps: 238 - Eps: 0.0100 - Time: 38.34s\n",
      "[EP 311/500] - Reward: 260.5305 - Steps: 490 - Eps: 0.0100 - Time: 75.41s\n",
      "[EP 312/500] - Reward: 10.1688 - Steps: 120 - Eps: 0.0100 - Time: 19.12s\n",
      "[EP 313/500] - Reward: 275.5614 - Steps: 232 - Eps: 0.0100 - Time: 37.60s\n",
      "[EP 314/500] - Reward: 304.5552 - Steps: 260 - Eps: 0.0100 - Time: 40.12s\n",
      "[EP 315/500] - Reward: 264.9682 - Steps: 180 - Eps: 0.0100 - Time: 27.88s\n",
      "[EP 316/500] - Reward: 23.4109 - Steps: 123 - Eps: 0.0100 - Time: 19.24s\n",
      "[EP 317/500] - Reward: 5.8858 - Steps: 105 - Eps: 0.0100 - Time: 16.03s\n",
      "[EP 318/500] - Reward: 14.6890 - Steps: 171 - Eps: 0.0100 - Time: 27.42s\n",
      "[EP 319/500] - Reward: 51.3418 - Steps: 154 - Eps: 0.0100 - Time: 24.08s\n",
      "[EP 320/500] - Reward: 142.0199 - Steps: 1000 - Eps: 0.0100 - Time: 155.11s\n",
      "[EP 321/500] - Reward: 226.3877 - Steps: 535 - Eps: 0.0100 - Time: 81.80s\n",
      "[EP 322/500] - Reward: -4.4729 - Steps: 231 - Eps: 0.0100 - Time: 35.84s\n",
      "[EP 323/500] - Reward: 240.6618 - Steps: 319 - Eps: 0.0100 - Time: 50.21s\n",
      "[EP 324/500] - Reward: 6.6475 - Steps: 206 - Eps: 0.0100 - Time: 32.15s\n",
      "[EP 325/500] - Reward: -238.4670 - Steps: 198 - Eps: 0.0100 - Time: 30.57s\n",
      "[EP 326/500] - Reward: -26.6944 - Steps: 264 - Eps: 0.0100 - Time: 41.07s\n",
      "[EP 327/500] - Reward: 100.4302 - Steps: 1000 - Eps: 0.0100 - Time: 155.11s\n",
      "[EP 328/500] - Reward: -396.6256 - Steps: 716 - Eps: 0.0100 - Time: 114.18s\n",
      "[EP 329/500] - Reward: 300.1959 - Steps: 676 - Eps: 0.0100 - Time: 104.57s\n",
      "[EP 330/500] - Reward: 294.7948 - Steps: 488 - Eps: 0.0100 - Time: 75.43s\n",
      "[EP 331/500] - Reward: 172.4763 - Steps: 1000 - Eps: 0.0100 - Time: 154.92s\n",
      "[EP 332/500] - Reward: -52.7626 - Steps: 97 - Eps: 0.0100 - Time: 15.03s\n",
      "[EP 333/500] - Reward: 22.0946 - Steps: 125 - Eps: 0.0100 - Time: 19.39s\n",
      "[EP 334/500] - Reward: 21.7606 - Steps: 1000 - Eps: 0.0100 - Time: 155.39s\n",
      "[EP 335/500] - Reward: 114.3521 - Steps: 1000 - Eps: 0.0100 - Time: 161.49s\n",
      "[EP 336/500] - Reward: -19.7120 - Steps: 173 - Eps: 0.0100 - Time: 27.69s\n",
      "[EP 337/500] - Reward: 291.6949 - Steps: 317 - Eps: 0.0100 - Time: 51.43s\n",
      "[EP 338/500] - Reward: 255.8924 - Steps: 830 - Eps: 0.0100 - Time: 132.12s\n",
      "[EP 339/500] - Reward: -2.7270 - Steps: 182 - Eps: 0.0100 - Time: 28.86s\n",
      "[EP 340/500] - Reward: 213.8271 - Steps: 682 - Eps: 0.0100 - Time: 108.19s\n",
      "[EP 341/500] - Reward: 261.0206 - Steps: 434 - Eps: 0.0100 - Time: 68.96s\n",
      "[EP 342/500] - Reward: 243.4568 - Steps: 258 - Eps: 0.0100 - Time: 40.71s\n",
      "[EP 343/500] - Reward: 276.9440 - Steps: 338 - Eps: 0.0100 - Time: 53.11s\n",
      "[EP 344/500] - Reward: 300.7865 - Steps: 250 - Eps: 0.0100 - Time: 39.54s\n",
      "[EP 345/500] - Reward: 259.3229 - Steps: 372 - Eps: 0.0100 - Time: 68.03s\n",
      "[EP 346/500] - Reward: 236.5705 - Steps: 392 - Eps: 0.0100 - Time: 59.42s\n",
      "[EP 347/500] - Reward: 268.5675 - Steps: 276 - Eps: 0.0100 - Time: 42.60s\n",
      "[EP 348/500] - Reward: 248.0156 - Steps: 316 - Eps: 0.0100 - Time: 49.62s\n",
      "[EP 349/500] - Reward: 268.0093 - Steps: 252 - Eps: 0.0100 - Time: 40.13s\n",
      "[EP 350/500] - Reward: 288.4934 - Steps: 340 - Eps: 0.0100 - Time: 52.67s\n",
      "[EP 351/500] - Reward: 249.6047 - Steps: 234 - Eps: 0.0100 - Time: 37.92s\n",
      "[EP 352/500] - Reward: 250.5699 - Steps: 246 - Eps: 0.0100 - Time: 41.14s\n",
      "[EP 353/500] - Reward: 243.6268 - Steps: 773 - Eps: 0.0100 - Time: 133.75s\n",
      "[EP 354/500] - Reward: 16.6299 - Steps: 152 - Eps: 0.0100 - Time: 26.92s\n",
      "[EP 355/500] - Reward: 290.3362 - Steps: 212 - Eps: 0.0100 - Time: 38.24s\n",
      "[EP 356/500] - Reward: 300.6910 - Steps: 291 - Eps: 0.0100 - Time: 52.00s\n",
      "[EP 357/500] - Reward: 289.8173 - Steps: 244 - Eps: 0.0100 - Time: 44.42s\n",
      "[EP 358/500] - Reward: 242.8153 - Steps: 215 - Eps: 0.0100 - Time: 39.80s\n",
      "[EP 359/500] - Reward: 220.1505 - Steps: 371 - Eps: 0.0100 - Time: 67.43s\n",
      "[EP 360/500] - Reward: 250.8531 - Steps: 197 - Eps: 0.0100 - Time: 35.36s\n",
      "[EP 361/500] - Reward: 278.2539 - Steps: 357 - Eps: 0.0100 - Time: 64.34s\n",
      "[EP 362/500] - Reward: 276.9174 - Steps: 243 - Eps: 0.0100 - Time: 43.64s\n",
      "[EP 363/500] - Reward: 253.8553 - Steps: 188 - Eps: 0.0100 - Time: 33.16s\n",
      "[EP 364/500] - Reward: 276.1284 - Steps: 250 - Eps: 0.0100 - Time: 45.99s\n",
      "[EP 365/500] - Reward: 260.4887 - Steps: 176 - Eps: 0.0100 - Time: 32.26s\n",
      "[EP 366/500] - Reward: 235.2117 - Steps: 282 - Eps: 0.0100 - Time: 50.05s\n",
      "[EP 367/500] - Reward: 278.6600 - Steps: 337 - Eps: 0.0100 - Time: 60.47s\n",
      "[EP 368/500] - Reward: 260.8355 - Steps: 187 - Eps: 0.0100 - Time: 33.76s\n",
      "[EP 369/500] - Reward: 259.3499 - Steps: 426 - Eps: 0.0100 - Time: 76.92s\n",
      "[EP 370/500] - Reward: 247.5547 - Steps: 360 - Eps: 0.0100 - Time: 65.07s\n",
      "[EP 371/500] - Reward: 287.1064 - Steps: 179 - Eps: 0.0100 - Time: 32.65s\n",
      "[EP 372/500] - Reward: 45.3320 - Steps: 81 - Eps: 0.0100 - Time: 14.08s\n",
      "[EP 373/500] - Reward: 208.9308 - Steps: 218 - Eps: 0.0100 - Time: 40.01s\n",
      "[EP 374/500] - Reward: 301.6777 - Steps: 205 - Eps: 0.0100 - Time: 36.85s\n",
      "[EP 375/500] - Reward: 273.4507 - Steps: 305 - Eps: 0.0100 - Time: 54.71s\n",
      "[EP 376/500] - Reward: 247.4764 - Steps: 314 - Eps: 0.0100 - Time: 56.63s\n",
      "[EP 377/500] - Reward: -33.1475 - Steps: 236 - Eps: 0.0100 - Time: 43.49s\n",
      "[EP 378/500] - Reward: 271.2884 - Steps: 264 - Eps: 0.0100 - Time: 46.52s\n",
      "[EP 379/500] - Reward: -187.2007 - Steps: 205 - Eps: 0.0100 - Time: 36.33s\n",
      "[EP 380/500] - Reward: 260.7346 - Steps: 360 - Eps: 0.0100 - Time: 64.85s\n",
      "[EP 381/500] - Reward: 265.8715 - Steps: 472 - Eps: 0.0100 - Time: 83.75s\n",
      "[EP 382/500] - Reward: 277.5077 - Steps: 312 - Eps: 0.0100 - Time: 55.86s\n",
      "[EP 383/500] - Reward: 290.6094 - Steps: 483 - Eps: 0.0100 - Time: 86.84s\n",
      "[EP 384/500] - Reward: 120.4150 - Steps: 1000 - Eps: 0.0100 - Time: 177.91s\n",
      "[EP 385/500] - Reward: 242.4733 - Steps: 319 - Eps: 0.0100 - Time: 57.64s\n",
      "[EP 386/500] - Reward: 266.4728 - Steps: 260 - Eps: 0.0100 - Time: 45.63s\n",
      "[EP 387/500] - Reward: 117.8803 - Steps: 1000 - Eps: 0.0100 - Time: 175.70s\n",
      "[EP 388/500] - Reward: 257.5583 - Steps: 655 - Eps: 0.0100 - Time: 113.01s\n",
      "[EP 389/500] - Reward: 185.7662 - Steps: 244 - Eps: 0.0100 - Time: 42.02s\n",
      "[EP 390/500] - Reward: 293.9494 - Steps: 189 - Eps: 0.0100 - Time: 32.08s\n",
      "[EP 391/500] - Reward: 219.7457 - Steps: 233 - Eps: 0.0100 - Time: 39.11s\n",
      "[EP 392/500] - Reward: 282.7204 - Steps: 180 - Eps: 0.0100 - Time: 31.17s\n",
      "[EP 393/500] - Reward: 244.9744 - Steps: 190 - Eps: 0.0100 - Time: 32.33s\n",
      "[EP 394/500] - Reward: 246.4122 - Steps: 179 - Eps: 0.0100 - Time: 29.67s\n",
      "[EP 395/500] - Reward: 16.7770 - Steps: 133 - Eps: 0.0100 - Time: 23.04s\n",
      "[EP 396/500] - Reward: 214.6470 - Steps: 268 - Eps: 0.0100 - Time: 44.56s\n",
      "[EP 397/500] - Reward: 285.5692 - Steps: 235 - Eps: 0.0100 - Time: 39.12s\n",
      "[EP 398/500] - Reward: 262.7117 - Steps: 320 - Eps: 0.0100 - Time: 54.98s\n",
      "[EP 399/500] - Reward: 261.5258 - Steps: 192 - Eps: 0.0100 - Time: 32.50s\n",
      "[EP 400/500] - Reward: 250.8932 - Steps: 364 - Eps: 0.0100 - Time: 69.65s\n",
      "[EP 401/500] - Reward: 246.2498 - Steps: 283 - Eps: 0.0100 - Time: 47.79s\n",
      "[EP 402/500] - Reward: 275.1834 - Steps: 205 - Eps: 0.0100 - Time: 33.42s\n",
      "[EP 403/500] - Reward: 26.2502 - Steps: 143 - Eps: 0.0100 - Time: 22.56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 05:04:23.156330: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 404/500] - Reward: -35.6143 - Steps: 1000 - Eps: 0.0100 - Time: 157.13s\n",
      "[EP 405/500] - Reward: 239.3493 - Steps: 225 - Eps: 0.0100 - Time: 35.71s\n",
      "[EP 406/500] - Reward: 257.4426 - Steps: 207 - Eps: 0.0100 - Time: 31.90s\n",
      "[EP 407/500] - Reward: 289.8971 - Steps: 251 - Eps: 0.0100 - Time: 39.07s\n",
      "[EP 408/500] - Reward: 227.9770 - Steps: 211 - Eps: 0.0100 - Time: 32.41s\n",
      "[EP 409/500] - Reward: 290.6784 - Steps: 228 - Eps: 0.0100 - Time: 35.86s\n",
      "[EP 410/500] - Reward: -139.9821 - Steps: 169 - Eps: 0.0100 - Time: 27.31s\n",
      "[EP 411/500] - Reward: -324.4651 - Steps: 85 - Eps: 0.0100 - Time: 12.98s\n",
      "[EP 412/500] - Reward: -67.9969 - Steps: 117 - Eps: 0.0100 - Time: 18.36s\n",
      "[EP 413/500] - Reward: 275.5010 - Steps: 184 - Eps: 0.0100 - Time: 29.23s\n",
      "[EP 414/500] - Reward: 6.9703 - Steps: 117 - Eps: 0.0100 - Time: 17.55s\n",
      "[EP 415/500] - Reward: -214.8524 - Steps: 125 - Eps: 0.0100 - Time: 20.03s\n",
      "[EP 416/500] - Reward: 296.1515 - Steps: 232 - Eps: 0.0100 - Time: 36.20s\n",
      "[EP 417/500] - Reward: 267.4782 - Steps: 300 - Eps: 0.0100 - Time: 46.63s\n",
      "[EP 418/500] - Reward: 212.9574 - Steps: 317 - Eps: 0.0100 - Time: 49.53s\n",
      "[EP 419/500] - Reward: -9.5836 - Steps: 101 - Eps: 0.0100 - Time: 15.73s\n",
      "[EP 420/500] - Reward: -62.4046 - Steps: 102 - Eps: 0.0100 - Time: 15.77s\n",
      "[EP 421/500] - Reward: -65.7827 - Steps: 96 - Eps: 0.0100 - Time: 15.31s\n",
      "[EP 422/500] - Reward: -37.0593 - Steps: 144 - Eps: 0.0100 - Time: 21.95s\n",
      "[EP 423/500] - Reward: 257.6773 - Steps: 273 - Eps: 0.0100 - Time: 44.50s\n",
      "[EP 424/500] - Reward: 124.9035 - Steps: 1000 - Eps: 0.0100 - Time: 158.74s\n",
      "[EP 425/500] - Reward: 279.3478 - Steps: 252 - Eps: 0.0100 - Time: 39.12s\n",
      "[EP 426/500] - Reward: 231.1679 - Steps: 336 - Eps: 0.0100 - Time: 54.51s\n",
      "[EP 427/500] - Reward: 250.9957 - Steps: 304 - Eps: 0.0100 - Time: 48.83s\n",
      "[EP 428/500] - Reward: 114.3477 - Steps: 1000 - Eps: 0.0100 - Time: 158.33s\n",
      "[EP 429/500] - Reward: 300.2978 - Steps: 387 - Eps: 0.0100 - Time: 62.96s\n",
      "[EP 430/500] - Reward: 256.4709 - Steps: 845 - Eps: 0.0100 - Time: 130.53s\n",
      "[EP 431/500] - Reward: 41.0846 - Steps: 166 - Eps: 0.0100 - Time: 25.35s\n",
      "[EP 432/500] - Reward: 233.1049 - Steps: 323 - Eps: 0.0100 - Time: 49.57s\n",
      "[EP 433/500] - Reward: 269.0567 - Steps: 458 - Eps: 0.0100 - Time: 70.91s\n",
      "[EP 434/500] - Reward: 275.2422 - Steps: 327 - Eps: 0.0100 - Time: 50.42s\n",
      "[EP 435/500] - Reward: 270.8173 - Steps: 437 - Eps: 0.0100 - Time: 67.21s\n",
      "[EP 436/500] - Reward: 263.9355 - Steps: 504 - Eps: 0.0100 - Time: 77.24s\n",
      "[EP 437/500] - Reward: 252.7869 - Steps: 254 - Eps: 0.0100 - Time: 40.38s\n",
      "[EP 438/500] - Reward: 49.3833 - Steps: 192 - Eps: 0.0100 - Time: 30.69s\n",
      "[EP 439/500] - Reward: 269.3946 - Steps: 200 - Eps: 0.0100 - Time: 30.79s\n",
      "[EP 440/500] - Reward: 236.6316 - Steps: 244 - Eps: 0.0100 - Time: 37.90s\n",
      "[EP 441/500] - Reward: 290.8103 - Steps: 353 - Eps: 0.0100 - Time: 55.12s\n",
      "[EP 442/500] - Reward: 292.5613 - Steps: 244 - Eps: 0.0100 - Time: 37.55s\n",
      "[EP 443/500] - Reward: 287.9577 - Steps: 467 - Eps: 0.0100 - Time: 73.09s\n",
      "[EP 444/500] - Reward: 247.6085 - Steps: 651 - Eps: 0.0100 - Time: 100.65s\n",
      "[EP 445/500] - Reward: 259.6452 - Steps: 242 - Eps: 0.0100 - Time: 37.89s\n",
      "[EP 446/500] - Reward: 280.8307 - Steps: 261 - Eps: 0.0100 - Time: 40.62s\n",
      "[EP 447/500] - Reward: 256.3625 - Steps: 198 - Eps: 0.0100 - Time: 31.27s\n",
      "[EP 448/500] - Reward: 298.8326 - Steps: 231 - Eps: 0.0100 - Time: 35.54s\n",
      "[EP 449/500] - Reward: 265.8652 - Steps: 249 - Eps: 0.0100 - Time: 37.99s\n",
      "[EP 450/500] - Reward: 247.7615 - Steps: 611 - Eps: 0.0100 - Time: 94.47s\n",
      "[EP 451/500] - Reward: 266.3058 - Steps: 257 - Eps: 0.0100 - Time: 39.33s\n",
      "[EP 452/500] - Reward: 280.8330 - Steps: 223 - Eps: 0.0100 - Time: 34.96s\n",
      "[EP 453/500] - Reward: 239.0430 - Steps: 389 - Eps: 0.0100 - Time: 60.86s\n",
      "[EP 454/500] - Reward: 269.7204 - Steps: 276 - Eps: 0.0100 - Time: 42.90s\n",
      "[EP 455/500] - Reward: 271.5895 - Steps: 208 - Eps: 0.0100 - Time: 32.75s\n",
      "[EP 456/500] - Reward: 286.0382 - Steps: 207 - Eps: 0.0100 - Time: 32.42s\n",
      "[EP 457/500] - Reward: 258.9890 - Steps: 151 - Eps: 0.0100 - Time: 22.59s\n",
      "[EP 458/500] - Reward: 289.6636 - Steps: 243 - Eps: 0.0100 - Time: 38.43s\n",
      "[EP 459/500] - Reward: 266.0510 - Steps: 218 - Eps: 0.0100 - Time: 33.97s\n",
      "[EP 460/500] - Reward: 307.7071 - Steps: 273 - Eps: 0.0100 - Time: 42.28s\n",
      "[EP 461/500] - Reward: 240.7703 - Steps: 359 - Eps: 0.0100 - Time: 56.79s\n",
      "[EP 462/500] - Reward: 208.6633 - Steps: 557 - Eps: 0.0100 - Time: 85.96s\n",
      "[EP 463/500] - Reward: 27.2226 - Steps: 85 - Eps: 0.0100 - Time: 13.20s\n",
      "[EP 464/500] - Reward: 272.1669 - Steps: 246 - Eps: 0.0100 - Time: 38.38s\n",
      "[EP 465/500] - Reward: 287.0988 - Steps: 258 - Eps: 0.0100 - Time: 40.14s\n",
      "[EP 466/500] - Reward: 279.8579 - Steps: 264 - Eps: 0.0100 - Time: 41.10s\n",
      "[EP 467/500] - Reward: 235.3068 - Steps: 819 - Eps: 0.0100 - Time: 127.73s\n",
      "[EP 468/500] - Reward: 254.1294 - Steps: 222 - Eps: 0.0100 - Time: 34.70s\n",
      "[EP 469/500] - Reward: 250.6847 - Steps: 307 - Eps: 0.0100 - Time: 47.61s\n",
      "[EP 470/500] - Reward: 271.4532 - Steps: 241 - Eps: 0.0100 - Time: 37.63s\n",
      "[EP 471/500] - Reward: 261.4165 - Steps: 233 - Eps: 0.0100 - Time: 35.86s\n",
      "[EP 472/500] - Reward: 269.8831 - Steps: 282 - Eps: 0.0100 - Time: 45.51s\n",
      "[EP 473/500] - Reward: 261.0502 - Steps: 248 - Eps: 0.0100 - Time: 39.30s\n",
      "[EP 474/500] - Reward: -123.1684 - Steps: 176 - Eps: 0.0100 - Time: 28.20s\n",
      "[EP 475/500] - Reward: 235.1252 - Steps: 649 - Eps: 0.0100 - Time: 102.25s\n",
      "[EP 476/500] - Reward: -36.5024 - Steps: 91 - Eps: 0.0100 - Time: 14.03s\n",
      "[EP 477/500] - Reward: 293.1955 - Steps: 479 - Eps: 0.0100 - Time: 75.40s\n",
      "[EP 478/500] - Reward: 43.9411 - Steps: 178 - Eps: 0.0100 - Time: 27.52s\n",
      "[EP 479/500] - Reward: -19.9096 - Steps: 111 - Eps: 0.0100 - Time: 17.31s\n",
      "[EP 480/500] - Reward: 154.5409 - Steps: 1000 - Eps: 0.0100 - Time: 155.74s\n",
      "[EP 481/500] - Reward: -220.9401 - Steps: 271 - Eps: 0.0100 - Time: 41.87s\n",
      "[EP 482/500] - Reward: 278.1922 - Steps: 282 - Eps: 0.0100 - Time: 43.56s\n",
      "[EP 483/500] - Reward: 325.1541 - Steps: 201 - Eps: 0.0100 - Time: 31.29s\n",
      "[EP 484/500] - Reward: 259.9167 - Steps: 929 - Eps: 0.0100 - Time: 144.82s\n",
      "[EP 485/500] - Reward: 216.1016 - Steps: 631 - Eps: 0.0100 - Time: 97.89s\n",
      "[EP 486/500] - Reward: 294.4566 - Steps: 460 - Eps: 0.0100 - Time: 71.83s\n",
      "[EP 487/500] - Reward: 254.3175 - Steps: 342 - Eps: 0.0100 - Time: 54.09s\n",
      "[EP 488/500] - Reward: 267.6497 - Steps: 407 - Eps: 0.0100 - Time: 63.48s\n",
      "[EP 489/500] - Reward: 273.6021 - Steps: 336 - Eps: 0.0100 - Time: 52.41s\n",
      "[EP 490/500] - Reward: 162.5256 - Steps: 720 - Eps: 0.0100 - Time: 111.19s\n",
      "[EP 491/500] - Reward: 313.7551 - Steps: 253 - Eps: 0.0100 - Time: 39.25s\n",
      "[EP 492/500] - Reward: 309.7030 - Steps: 408 - Eps: 0.0100 - Time: 63.24s\n",
      "[EP 493/500] - Reward: 228.7043 - Steps: 226 - Eps: 0.0100 - Time: 34.65s\n",
      "[EP 494/500] - Reward: 262.1128 - Steps: 196 - Eps: 0.0100 - Time: 31.45s\n",
      "[EP 495/500] - Reward: 154.6374 - Steps: 1000 - Eps: 0.0100 - Time: 156.74s\n",
      "[EP 496/500] - Reward: 233.0389 - Steps: 257 - Eps: 0.0100 - Time: 39.97s\n",
      "[EP 497/500] - Reward: 255.2236 - Steps: 177 - Eps: 0.0100 - Time: 27.73s\n",
      "[EP 498/500] - Reward: 287.6807 - Steps: 218 - Eps: 0.0100 - Time: 33.84s\n",
      "[EP 499/500] - Reward: 239.8510 - Steps: 235 - Eps: 0.0100 - Time: 36.59s\n",
      "[EP 500/500] - Reward: -37.8855 - Steps: 113 - Eps: 0.0100 - Time: 20.04s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c872d5e1284e37ad9073f2d3749ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward</td><td>▁▄▄▄▅▅▅▅▄▅▅▆▆▇▆▅█▆▇▅███▆█▇▅██▆▇▇▇▅█████▇</td></tr><tr><td>steps</td><td>▁▁▅▂███▁█▁▃██▄█▂▂█▃▁▃▂▃▂▂█▁▂▂▁█▂▂▁▄▂▂▃▇▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.01</td></tr><tr><td>reward</td><td>-37.88553</td></tr><tr><td>steps</td><td>113</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glistening-rat-63</strong>: <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/27aql4y9\" target=\"_blank\">https://wandb.ai/timothyckl/lunar-lander-rl/runs/27aql4y9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230127_204657-27aql4y9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = DQL(env, learning_rate, discount_factor, exploration_rate)\n",
    "history = agent.train(episodes, max_steps, log_wandb=True, save_episodes=True, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6293d363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-28T06:26:03.214571Z",
     "iopub.status.busy": "2023-01-28T06:26:03.213689Z",
     "iopub.status.idle": "2023-01-28T06:26:03.221850Z",
     "shell.execute_reply": "2023-01-28T06:26:03.221035Z"
    },
    "papermill": {
     "duration": 0.041771,
     "end_time": "2023-01-28T06:26:03.223938",
     "exception": false,
     "start_time": "2023-01-28T06:26:03.182167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save history\n",
    "if not os.path.exists('./history'):\n",
    "        os.mkdir('./history')\n",
    "\n",
    "with open('./history/dqn_history.json', 'w') as file:\n",
    "    json.dump(history, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34793.342053,
   "end_time": "2023-01-28T06:26:06.222611",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-27T20:46:12.880558",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "10f74871797546f8b54ce56ea54c8638": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "49b1615f98434c5696197e88ffb1a3dc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "77af897b574f4b07951702bb6164a6c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "951a85e8a37546288e9a7e7d026ad1f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_77af897b574f4b07951702bb6164a6c0",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fa56a260d29444a6bc92a6386f2d232a",
       "value": 1
      }
     },
     "b7c872d5e1284e37ad9073f2d3749ee4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d55ede915d7047b8be51ab1c74e80cc8",
        "IPY_MODEL_951a85e8a37546288e9a7e7d026ad1f3"
       ],
       "layout": "IPY_MODEL_10f74871797546f8b54ce56ea54c8638"
      }
     },
     "d55ede915d7047b8be51ab1c74e80cc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "LabelModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "LabelView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_49b1615f98434c5696197e88ffb1a3dc",
       "placeholder": "​",
       "style": "IPY_MODEL_e5193cc5e84245b78dbd3225c0861f26",
       "value": "0.077 MB of 0.077 MB uploaded (0.000 MB deduped)\r"
      }
     },
     "e5193cc5e84245b78dbd3225c0861f26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fa56a260d29444a6bc92a6386f2d232a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
