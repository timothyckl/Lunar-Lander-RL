{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from models.dqn import DQN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', continuous=False, render_mode='rgb_array')\n",
    "\n",
    "discount = 0.99\n",
    "learning_rate = 0.001\n",
    "exploration = 1.0\n",
    "exploration_decay = 0.99\n",
    "n_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 0/1000]  Rewards: -322.3453 | Steps: 101 | Eps: 1.0000 | Time: 4.8605s\n",
      "[EP 1/1000]  Rewards: -151.4424 | Steps: 89 | Eps: 0.9900 | Time: 9.2431s\n",
      "[EP 2/1000]  Rewards: -407.6679 | Steps: 103 | Eps: 0.9801 | Time: 10.1630s\n",
      "[EP 3/1000]  Rewards: -124.8096 | Steps: 86 | Eps: 0.9703 | Time: 8.5653s\n",
      "[EP 4/1000]  Rewards: -305.7821 | Steps: 105 | Eps: 0.9606 | Time: 10.7305s\n",
      "[EP 5/1000]  Rewards: -135.8642 | Steps: 93 | Eps: 0.9510 | Time: 10.2825s\n",
      "[EP 6/1000]  Rewards: -84.9237 | Steps: 66 | Eps: 0.9415 | Time: 6.8369s\n",
      "[EP 7/1000]  Rewards: -164.0676 | Steps: 95 | Eps: 0.9321 | Time: 10.8511s\n",
      "[EP 8/1000]  Rewards: -104.5514 | Steps: 64 | Eps: 0.9227 | Time: 6.6832s\n",
      "[EP 9/1000]  Rewards: -105.0931 | Steps: 106 | Eps: 0.9135 | Time: 10.9048s\n",
      "[EP 10/1000]  Rewards: -105.7492 | Steps: 63 | Eps: 0.9044 | Time: 6.6779s\n",
      "[EP 11/1000]  Rewards: -434.0916 | Steps: 156 | Eps: 0.8953 | Time: 16.8445s\n",
      "[EP 12/1000]  Rewards: -238.8429 | Steps: 108 | Eps: 0.8864 | Time: 11.5633s\n",
      "[EP 13/1000]  Rewards: -137.7732 | Steps: 124 | Eps: 0.8775 | Time: 13.0275s\n",
      "[EP 14/1000]  Rewards: -142.6784 | Steps: 83 | Eps: 0.8687 | Time: 8.7694s\n",
      "[EP 15/1000]  Rewards: -122.1319 | Steps: 68 | Eps: 0.8601 | Time: 7.0638s\n",
      "[EP 16/1000]  Rewards: -70.4718 | Steps: 73 | Eps: 0.8515 | Time: 7.5674s\n",
      "[EP 17/1000]  Rewards: -208.6220 | Steps: 94 | Eps: 0.8429 | Time: 9.8812s\n",
      "[EP 18/1000]  Rewards: -169.7933 | Steps: 63 | Eps: 0.8345 | Time: 7.2996s\n",
      "[EP 19/1000]  Rewards: -85.9532 | Steps: 107 | Eps: 0.8262 | Time: 11.4658s\n",
      "[EP 20/1000]  Rewards: -186.7860 | Steps: 110 | Eps: 0.8179 | Time: 11.7573s\n",
      "[EP 21/1000]  Rewards: -375.1697 | Steps: 96 | Eps: 0.8097 | Time: 10.0690s\n",
      "[EP 22/1000]  Rewards: -382.5774 | Steps: 80 | Eps: 0.8016 | Time: 8.5765s\n",
      "[EP 23/1000]  Rewards: -108.1126 | Steps: 95 | Eps: 0.7936 | Time: 10.5467s\n",
      "[EP 24/1000]  Rewards: -127.0977 | Steps: 92 | Eps: 0.7857 | Time: 9.8834s\n",
      "[EP 25/1000]  Rewards: -69.6148 | Steps: 70 | Eps: 0.7778 | Time: 8.6905s\n",
      "[EP 26/1000]  Rewards: -163.0143 | Steps: 96 | Eps: 0.7700 | Time: 10.3619s\n",
      "[EP 27/1000]  Rewards: -228.7704 | Steps: 73 | Eps: 0.7623 | Time: 7.6961s\n",
      "[EP 28/1000]  Rewards: -212.0491 | Steps: 115 | Eps: 0.7547 | Time: 12.2060s\n",
      "[EP 29/1000]  Rewards: -259.5132 | Steps: 66 | Eps: 0.7472 | Time: 7.1433s\n",
      "[EP 30/1000]  Rewards: -112.8857 | Steps: 77 | Eps: 0.7397 | Time: 8.3800s\n",
      "[EP 31/1000]  Rewards: -106.1889 | Steps: 66 | Eps: 0.7323 | Time: 6.9835s\n",
      "[EP 32/1000]  Rewards: -92.5650 | Steps: 61 | Eps: 0.7250 | Time: 7.1169s\n",
      "[EP 33/1000]  Rewards: -87.9771 | Steps: 69 | Eps: 0.7177 | Time: 8.0523s\n",
      "[EP 34/1000]  Rewards: -317.8216 | Steps: 70 | Eps: 0.7106 | Time: 7.6851s\n",
      "[EP 35/1000]  Rewards: -441.9914 | Steps: 95 | Eps: 0.7034 | Time: 10.5917s\n",
      "[EP 36/1000]  Rewards: -490.2795 | Steps: 108 | Eps: 0.6964 | Time: 11.3503s\n",
      "[EP 37/1000]  Rewards: -186.7719 | Steps: 72 | Eps: 0.6894 | Time: 7.5500s\n",
      "[EP 38/1000]  Rewards: -421.8344 | Steps: 122 | Eps: 0.6826 | Time: 12.8065s\n",
      "[EP 39/1000]  Rewards: -134.0566 | Steps: 79 | Eps: 0.6757 | Time: 9.1378s\n",
      "[EP 40/1000]  Rewards: -163.6337 | Steps: 64 | Eps: 0.6690 | Time: 7.0440s\n",
      "[EP 41/1000]  Rewards: -476.6111 | Steps: 105 | Eps: 0.6623 | Time: 11.1925s\n",
      "[EP 42/1000]  Rewards: -409.9119 | Steps: 85 | Eps: 0.6557 | Time: 9.1010s\n",
      "[EP 43/1000]  Rewards: -488.3596 | Steps: 87 | Eps: 0.6491 | Time: 9.5176s\n",
      "[EP 44/1000]  Rewards: -181.7253 | Steps: 58 | Eps: 0.6426 | Time: 6.3950s\n",
      "[EP 45/1000]  Rewards: -471.8923 | Steps: 101 | Eps: 0.6362 | Time: 11.7709s\n",
      "[EP 46/1000]  Rewards: -365.0863 | Steps: 87 | Eps: 0.6298 | Time: 10.0748s\n",
      "[EP 47/1000]  Rewards: -425.5973 | Steps: 78 | Eps: 0.6235 | Time: 9.1465s\n",
      "[EP 48/1000]  Rewards: -602.0928 | Steps: 86 | Eps: 0.6173 | Time: 10.7829s\n",
      "[EP 49/1000]  Rewards: -324.9686 | Steps: 65 | Eps: 0.6111 | Time: 7.6456s\n",
      "[EP 50/1000]  Rewards: -431.4864 | Steps: 69 | Eps: 0.6050 | Time: 7.7237s\n",
      "[EP 51/1000]  Rewards: -373.2423 | Steps: 77 | Eps: 0.5990 | Time: 8.3311s\n",
      "[EP 52/1000]  Rewards: -375.7863 | Steps: 60 | Eps: 0.5930 | Time: 6.5456s\n",
      "[EP 53/1000]  Rewards: -482.2445 | Steps: 92 | Eps: 0.5870 | Time: 10.6185s\n",
      "[EP 54/1000]  Rewards: -427.6971 | Steps: 79 | Eps: 0.5812 | Time: 8.6065s\n",
      "[EP 55/1000]  Rewards: -373.2194 | Steps: 68 | Eps: 0.5754 | Time: 7.2110s\n",
      "[EP 56/1000]  Rewards: -33.7155 | Steps: 63 | Eps: 0.5696 | Time: 6.7563s\n",
      "[EP 57/1000]  Rewards: -359.9856 | Steps: 70 | Eps: 0.5639 | Time: 7.8011s\n",
      "[EP 58/1000]  Rewards: -552.8049 | Steps: 89 | Eps: 0.5583 | Time: 9.6096s\n",
      "[EP 59/1000]  Rewards: -169.1777 | Steps: 60 | Eps: 0.5527 | Time: 6.8603s\n",
      "[EP 60/1000]  Rewards: -545.3610 | Steps: 96 | Eps: 0.5472 | Time: 10.7402s\n",
      "[EP 61/1000]  Rewards: -317.0337 | Steps: 77 | Eps: 0.5417 | Time: 9.0657s\n",
      "[EP 62/1000]  Rewards: -504.0912 | Steps: 82 | Eps: 0.5363 | Time: 8.9211s\n",
      "[EP 63/1000]  Rewards: -490.8096 | Steps: 88 | Eps: 0.5309 | Time: 9.6460s\n",
      "[EP 64/1000]  Rewards: -235.2504 | Steps: 59 | Eps: 0.5256 | Time: 6.3159s\n",
      "[EP 65/1000]  Rewards: -485.7476 | Steps: 73 | Eps: 0.5203 | Time: 8.0111s\n",
      "[EP 66/1000]  Rewards: -472.1533 | Steps: 76 | Eps: 0.5151 | Time: 9.3457s\n",
      "[EP 67/1000]  Rewards: -345.4440 | Steps: 78 | Eps: 0.5100 | Time: 8.4724s\n",
      "[EP 68/1000]  Rewards: -442.3164 | Steps: 88 | Eps: 0.5049 | Time: 9.7345s\n",
      "[EP 69/1000]  Rewards: -660.4787 | Steps: 91 | Eps: 0.4998 | Time: 10.2822s\n",
      "[EP 70/1000]  Rewards: -262.7678 | Steps: 64 | Eps: 0.4948 | Time: 7.1933s\n",
      "[EP 71/1000]  Rewards: -219.9080 | Steps: 55 | Eps: 0.4899 | Time: 6.0612s\n",
      "[EP 72/1000]  Rewards: -567.2555 | Steps: 88 | Eps: 0.4850 | Time: 9.5806s\n",
      "[EP 73/1000]  Rewards: -344.1003 | Steps: 75 | Eps: 0.4801 | Time: 8.3789s\n",
      "[EP 74/1000]  Rewards: -228.9133 | Steps: 82 | Eps: 0.4753 | Time: 9.1571s\n",
      "[EP 75/1000]  Rewards: -269.4583 | Steps: 71 | Eps: 0.4706 | Time: 7.9319s\n",
      "[EP 76/1000]  Rewards: -331.2868 | Steps: 80 | Eps: 0.4659 | Time: 9.5146s\n",
      "[EP 77/1000]  Rewards: -508.6554 | Steps: 70 | Eps: 0.4612 | Time: 8.0331s\n",
      "[EP 78/1000]  Rewards: -575.5100 | Steps: 75 | Eps: 0.4566 | Time: 8.7366s\n",
      "[EP 79/1000]  Rewards: -432.8409 | Steps: 72 | Eps: 0.4520 | Time: 9.7067s\n",
      "[EP 80/1000]  Rewards: -323.5504 | Steps: 87 | Eps: 0.4475 | Time: 9.4257s\n",
      "[EP 81/1000]  Rewards: -350.5654 | Steps: 86 | Eps: 0.4430 | Time: 9.4271s\n",
      "[EP 82/1000]  Rewards: 41.2114 | Steps: 103 | Eps: 0.4386 | Time: 11.5314s\n",
      "[EP 83/1000]  Rewards: -257.5676 | Steps: 92 | Eps: 0.4342 | Time: 12.7750s\n",
      "[EP 84/1000]  Rewards: -205.4585 | Steps: 64 | Eps: 0.4299 | Time: 7.8784s\n",
      "[EP 85/1000]  Rewards: -346.5537 | Steps: 99 | Eps: 0.4256 | Time: 12.8306s\n",
      "[EP 86/1000]  Rewards: -267.5331 | Steps: 108 | Eps: 0.4213 | Time: 13.4631s\n",
      "[EP 87/1000]  Rewards: -457.7542 | Steps: 95 | Eps: 0.4171 | Time: 11.4390s\n",
      "[EP 88/1000]  Rewards: -492.4846 | Steps: 101 | Eps: 0.4129 | Time: 12.2143s\n",
      "[EP 89/1000]  Rewards: -261.8651 | Steps: 89 | Eps: 0.4088 | Time: 10.5631s\n",
      "[EP 90/1000]  Rewards: -514.6571 | Steps: 95 | Eps: 0.4047 | Time: 10.9841s\n",
      "[EP 91/1000]  Rewards: -343.2203 | Steps: 109 | Eps: 0.4007 | Time: 12.2402s\n",
      "[EP 92/1000]  Rewards: -315.6191 | Steps: 116 | Eps: 0.3967 | Time: 14.0652s\n",
      "[EP 93/1000]  Rewards: 49.1633 | Steps: 123 | Eps: 0.3927 | Time: 14.1556s\n",
      "[EP 94/1000]  Rewards: -516.2023 | Steps: 110 | Eps: 0.3888 | Time: 12.8212s\n",
      "[EP 95/1000]  Rewards: -272.8065 | Steps: 162 | Eps: 0.3849 | Time: 20.6021s\n",
      "[EP 96/1000]  Rewards: -469.1728 | Steps: 142 | Eps: 0.3810 | Time: 16.2598s\n",
      "[EP 97/1000]  Rewards: -370.7137 | Steps: 205 | Eps: 0.3772 | Time: 23.8817s\n",
      "[EP 98/1000]  Rewards: -509.9302 | Steps: 83 | Eps: 0.3735 | Time: 9.4236s\n",
      "[EP 99/1000]  Rewards: -340.8066 | Steps: 97 | Eps: 0.3697 | Time: 11.0018s\n",
      "[EP 100/1000]  Rewards: -127.1636 | Steps: 149 | Eps: 0.3660 | Time: 16.6738s\n",
      "[EP 101/1000]  Rewards: -323.5510 | Steps: 101 | Eps: 0.3624 | Time: 11.4746s\n",
      "[EP 102/1000]  Rewards: -145.0501 | Steps: 109 | Eps: 0.3587 | Time: 13.5717s\n",
      "[EP 103/1000]  Rewards: -412.8642 | Steps: 84 | Eps: 0.3552 | Time: 9.5395s\n",
      "[EP 104/1000]  Rewards: -429.5827 | Steps: 74 | Eps: 0.3516 | Time: 8.5037s\n",
      "[EP 105/1000]  Rewards: -423.0611 | Steps: 81 | Eps: 0.3481 | Time: 9.1549s\n",
      "[EP 106/1000]  Rewards: -378.6465 | Steps: 98 | Eps: 0.3446 | Time: 10.9780s\n",
      "[EP 107/1000]  Rewards: -464.0557 | Steps: 77 | Eps: 0.3412 | Time: 8.8704s\n",
      "[EP 108/1000]  Rewards: -470.9130 | Steps: 148 | Eps: 0.3378 | Time: 17.3350s\n",
      "[EP 109/1000]  Rewards: -482.0162 | Steps: 241 | Eps: 0.3344 | Time: 29.0103s\n",
      "[EP 110/1000]  Rewards: -405.1320 | Steps: 102 | Eps: 0.3310 | Time: 12.0035s\n",
      "[EP 111/1000]  Rewards: -426.7335 | Steps: 112 | Eps: 0.3277 | Time: 14.8192s\n",
      "[EP 112/1000]  Rewards: -282.3580 | Steps: 85 | Eps: 0.3244 | Time: 10.4616s\n",
      "[EP 113/1000]  Rewards: -455.0523 | Steps: 107 | Eps: 0.3212 | Time: 12.2205s\n",
      "[EP 114/1000]  Rewards: -395.4225 | Steps: 88 | Eps: 0.3180 | Time: 10.0305s\n",
      "[EP 115/1000]  Rewards: -440.7956 | Steps: 197 | Eps: 0.3148 | Time: 22.2400s\n",
      "[EP 116/1000]  Rewards: -383.9386 | Steps: 134 | Eps: 0.3117 | Time: 16.0132s\n",
      "[EP 117/1000]  Rewards: -174.8378 | Steps: 152 | Eps: 0.3085 | Time: 17.0082s\n",
      "[EP 118/1000]  Rewards: -409.7638 | Steps: 95 | Eps: 0.3055 | Time: 11.6871s\n",
      "[EP 119/1000]  Rewards: -427.5211 | Steps: 222 | Eps: 0.3024 | Time: 26.9204s\n",
      "[EP 120/1000]  Rewards: -352.6630 | Steps: 95 | Eps: 0.2994 | Time: 13.5978s\n",
      "[EP 121/1000]  Rewards: -543.2444 | Steps: 311 | Eps: 0.2964 | Time: 36.4778s\n",
      "[EP 122/1000]  Rewards: -271.5696 | Steps: 90 | Eps: 0.2934 | Time: 12.8714s\n",
      "[EP 123/1000]  Rewards: -341.9499 | Steps: 153 | Eps: 0.2905 | Time: 18.9158s\n",
      "[EP 124/1000]  Rewards: -208.7790 | Steps: 1000 | Eps: 0.2876 | Time: 115.4019s\n",
      "[EP 125/1000]  Rewards: -347.5210 | Steps: 153 | Eps: 0.2847 | Time: 17.9582s\n",
      "[EP 126/1000]  Rewards: -418.7514 | Steps: 313 | Eps: 0.2819 | Time: 36.8403s\n",
      "[EP 127/1000]  Rewards: -418.3406 | Steps: 608 | Eps: 0.2790 | Time: 72.2923s\n",
      "[EP 128/1000]  Rewards: -309.3829 | Steps: 528 | Eps: 0.2763 | Time: 64.0441s\n",
      "[EP 129/1000]  Rewards: -459.5606 | Steps: 248 | Eps: 0.2735 | Time: 29.9382s\n",
      "[EP 130/1000]  Rewards: -306.8948 | Steps: 267 | Eps: 0.2708 | Time: 36.3697s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\timot\\Desktop\\DELE_CA2\\Reinforcement-Learning-CA2\\0_DQN.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/timot/Desktop/DELE_CA2/Reinforcement-Learning-CA2/0_DQN.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent \u001b[39m=\u001b[39m DQN(env, discount, learning_rate, exploration, exploration_decay)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/timot/Desktop/DELE_CA2/Reinforcement-Learning-CA2/0_DQN.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rewards, exploration_rates, steps_per_episode \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(n_episodes)\n",
      "File \u001b[1;32mc:\\Users\\timot\\Desktop\\DELE_CA2\\Reinforcement-Learning-CA2\\models\\dqn.py:117\u001b[0m, in \u001b[0;36mDQN.train\u001b[1;34m(self, n_episodes, update_qnets)\u001b[0m\n\u001b[0;32m    115\u001b[0m next_state, reward, done, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m    116\u001b[0m next_state \u001b[39m=\u001b[39m next_state\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_size)\n\u001b[1;32m--> 117\u001b[0m frames\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender())\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m update_qnets:\n\u001b[0;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mappend((state, action, reward, next_state, done))\n",
      "File \u001b[1;32mc:\\Users\\timot\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\gymnasium\\core.py:371\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RenderFrame \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[RenderFrame] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     \u001b[39m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[1;32mc:\\Users\\timot\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m     )\n\u001b[1;32m---> 51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\timot\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m env_render_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\timot\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:705\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    689\u001b[0m             pygame\u001b[39m.\u001b[39mdraw\u001b[39m.\u001b[39mline(\n\u001b[0;32m    690\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf,\n\u001b[0;32m    691\u001b[0m                 color\u001b[39m=\u001b[39m(\u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    694\u001b[0m                 width\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    695\u001b[0m             )\n\u001b[0;32m    696\u001b[0m             pygame\u001b[39m.\u001b[39mdraw\u001b[39m.\u001b[39mpolygon(\n\u001b[0;32m    697\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf,\n\u001b[0;32m    698\u001b[0m                 color\u001b[39m=\u001b[39m(\u001b[39m204\u001b[39m, \u001b[39m204\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    703\u001b[0m                 ],\n\u001b[0;32m    704\u001b[0m             )\n\u001b[1;32m--> 705\u001b[0m             gfxdraw\u001b[39m.\u001b[39;49maapolygon(\n\u001b[0;32m    706\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msurf,\n\u001b[0;32m    707\u001b[0m                 [(x, flagy2), (x, flagy2 \u001b[39m-\u001b[39;49m \u001b[39m10\u001b[39;49m), (x \u001b[39m+\u001b[39;49m \u001b[39m25\u001b[39;49m, flagy2 \u001b[39m-\u001b[39;49m \u001b[39m5\u001b[39;49m)],\n\u001b[0;32m    708\u001b[0m                 (\u001b[39m204\u001b[39;49m, \u001b[39m204\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m    709\u001b[0m             )\n\u001b[0;32m    711\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mflip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, \u001b[39mFalse\u001b[39;00m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQN(env, discount, learning_rate, exploration, exploration_decay)\n",
    "rewards, exploration_rates, steps_per_episode = agent.train(n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "axes[0].plot(rewards)\n",
    "axes[0].set_title('Rewards')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward')\n",
    "\n",
    "axes[1].plot(exploration_rates)\n",
    "axes[1].set_title('Exploration Rates')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Exploration Rate')\n",
    "\n",
    "axes[2].plot(steps_per_episode)\n",
    "axes[2].set_title('Steps per Episode')\n",
    "axes[2].set_xlabel('Episode')\n",
    "axes[2].set_ylabel('Steps')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Oct 19 2022, 22:38:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15c5baac91721a2e9125fc9e7830b5bd4b6688550f5daa42d124d7a05043362d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
