{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from models.dqn import Agent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "lr = 0.0004233541071584581\n",
    "discount = 0.99\n",
    "exploration_rate = 1.0\n",
    "exploration_decay = 0.9524257785303656\n",
    "update_interval = 10\n",
    "num_episodes = 1000\n",
    "agent = Agent(\n",
    "    env=env,\n",
    "    alpha=lr,\n",
    "    gamma=discount,\n",
    "    epsilon=exploration_rate,\n",
    "    epsilon_decay=exploration_decay,\n",
    "    update_interval=update_interval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Episode 1/1000]\n",
      "Reward: -158.6400   Avg Reward: -158.6400    Steps: 74    ER: 0.9524    Time: 0.2352s\n",
      "\n",
      "[Episode 2/1000]\n",
      "Reward: -232.7795   Avg Reward: -195.7098    Steps: 98    ER: 0.9071    Time: 14.6507s\n",
      "\n",
      "[Episode 3/1000]\n",
      "Reward: -154.3407   Avg Reward: -181.9201    Steps: 61    ER: 0.8640    Time: 11.5620s\n",
      "\n",
      "[Episode 4/1000]\n",
      "Reward: -212.2660   Avg Reward: -189.5066    Steps: 76    ER: 0.8229    Time: 14.5753s\n",
      "\n",
      "[Episode 5/1000]\n",
      "Reward: -113.8761   Avg Reward: -174.3805    Steps: 118    ER: 0.7837    Time: 23.0528s\n",
      "\n",
      "[Episode 6/1000]\n",
      "Reward: -193.9994   Avg Reward: -177.6503    Steps: 96    ER: 0.7464    Time: 18.9410s\n",
      "\n",
      "[Episode 7/1000]\n",
      "Reward: -121.8179   Avg Reward: -169.6742    Steps: 99    ER: 0.7109    Time: 19.1501s\n",
      "\n",
      "[Episode 8/1000]\n",
      "Reward: -120.5214   Avg Reward: -163.5301    Steps: 92    ER: 0.6771    Time: 18.0027s\n",
      "\n",
      "[Episode 9/1000]\n",
      "Reward: -27.9658   Avg Reward: -148.4674    Steps: 109    ER: 0.6449    Time: 21.6374s\n",
      "\n",
      "[Episode 10/1000]\n",
      "Reward: 16.3090   Avg Reward: -131.9898    Steps: 108    ER: 0.6142    Time: 21.1349s\n",
      "\n",
      "[Episode 11/1000]\n",
      "Reward: -256.0500   Avg Reward: -143.2680    Steps: 205    ER: 0.5850    Time: 40.9138s\n",
      "\n",
      "[Episode 12/1000]\n",
      "Reward: -306.1642   Avg Reward: -156.8427    Steps: 166    ER: 0.5572    Time: 34.1399s\n",
      "\n",
      "[Episode 13/1000]\n",
      "Reward: 0.2035   Avg Reward: -144.7622    Steps: 217    ER: 0.5306    Time: 44.8574s\n",
      "\n",
      "[Episode 14/1000]\n",
      "Reward: -104.9644   Avg Reward: -141.9195    Steps: 98    ER: 0.5054    Time: 20.2341s\n",
      "\n",
      "[Episode 15/1000]\n",
      "Reward: -4.3346   Avg Reward: -132.7472    Steps: 154    ER: 0.4814    Time: 32.5159s\n",
      "\n",
      "[Episode 16/1000]\n",
      "Reward: -38.7164   Avg Reward: -126.8702    Steps: 169    ER: 0.4585    Time: 35.5482s\n",
      "\n",
      "[Episode 17/1000]\n",
      "Reward: -86.6092   Avg Reward: -124.5019    Steps: 76    ER: 0.4366    Time: 16.1257s\n",
      "\n",
      "[Episode 18/1000]\n",
      "Reward: -53.7180   Avg Reward: -120.5695    Steps: 92    ER: 0.4159    Time: 19.6364s\n",
      "\n",
      "[Episode 19/1000]\n",
      "Reward: -11.5215   Avg Reward: -114.8301    Steps: 242    ER: 0.3961    Time: 53.2075s\n",
      "\n",
      "[Episode 20/1000]\n",
      "Reward: -29.2639   Avg Reward: -110.5518    Steps: 200    ER: 0.3772    Time: 43.5605s\n",
      "\n",
      "[Episode 21/1000]\n",
      "Reward: -13.7985   Avg Reward: -105.9445    Steps: 161    ER: 0.3593    Time: 34.3044s\n",
      "\n",
      "[Episode 22/1000]\n",
      "Reward: -53.6987   Avg Reward: -103.5697    Steps: 195    ER: 0.3422    Time: 41.8276s\n",
      "\n",
      "[Episode 23/1000]\n",
      "Reward: 14.7082   Avg Reward: -98.4272    Steps: 121    ER: 0.3259    Time: 26.3555s\n",
      "\n",
      "[Episode 24/1000]\n",
      "Reward: -232.1916   Avg Reward: -104.0007    Steps: 235    ER: 0.3104    Time: 50.4773s\n",
      "\n",
      "[Episode 25/1000]\n",
      "Reward: -306.4140   Avg Reward: -112.0972    Steps: 634    ER: 0.2957    Time: 138.3330s\n",
      "\n",
      "[Episode 26/1000]\n",
      "Reward: -78.1426   Avg Reward: -110.7913    Steps: 95    ER: 0.2816    Time: 20.5014s\n",
      "\n",
      "[Episode 27/1000]\n",
      "Reward: -185.6770   Avg Reward: -113.5648    Steps: 576    ER: 0.2682    Time: 122.9691s\n",
      "\n",
      "[Episode 28/1000]\n",
      "Reward: -567.1956   Avg Reward: -129.7659    Steps: 2417    ER: 0.2554    Time: 533.6425s\n"
     ]
    }
   ],
   "source": [
    "rewards, exploration_rate, steps_per_episode = agent.train(num_episodes=num_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "666a1b6055a6d64029d3ba184b27518fce88fba32d5a3ee2ba82c9dc2fa1e9d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
