{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import wandb\n",
    "import gymnasium as gym\n",
    "from models.dueling_dql import DuelingDQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\p2106911/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 5f71bc6f91cdaa551a70e88cf2522fcc1425d29b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimothyckl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\p2106911\\Desktop\\Reinforcement-Learning-CA2\\wandb\\run-20230127_145810-jpbajspm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/jpbajspm\" target=\"_blank\">abundant-fireworks-61</a></strong> to <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl\" target=\"_blank\">https://wandb.ai/timothyckl/lunar-lander-rl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/jpbajspm\" target=\"_blank\">https://wandb.ai/timothyckl/lunar-lander-rl/runs/jpbajspm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/timothyckl/lunar-lander-rl/runs/jpbajspm?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x26784fd2070>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', continuous=False, render_mode='rgb_array')\n",
    "learning_rate = 1e-3\n",
    "discount_factor = 0.99\n",
    "exploration_rate = 1.0\n",
    "episodes = 1000\n",
    "max_steps = 1000\n",
    "\n",
    "wandb.init(project='lunar-lander-rl', entity='timothyckl', config={\n",
    "    'learning_rate': learning_rate,\n",
    "    'discount_factor': discount_factor,\n",
    "    'exploration_rate': exploration_rate,\n",
    "    'episodes': episodes,\n",
    "    'max_steps': max_steps\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 1/1000] - Reward: -61.1582 - Steps: 67 - Eps: 0.0100 - Time: 2.81s\n",
      "[EP 2/1000] - Reward: -545.1042 - Steps: 116 - Eps: 0.0100 - Time: 3.26s\n",
      "[EP 3/1000] - Reward: -335.3770 - Steps: 324 - Eps: 0.0100 - Time: 9.05s\n",
      "[EP 4/1000] - Reward: -472.8986 - Steps: 115 - Eps: 0.0100 - Time: 3.31s\n",
      "[EP 5/1000] - Reward: -147.3898 - Steps: 156 - Eps: 0.0100 - Time: 4.45s\n",
      "[EP 6/1000] - Reward: -455.2603 - Steps: 76 - Eps: 0.0100 - Time: 2.17s\n",
      "[EP 7/1000] - Reward: -208.3863 - Steps: 143 - Eps: 0.0100 - Time: 4.03s\n",
      "[EP 8/1000] - Reward: -132.1474 - Steps: 238 - Eps: 0.0100 - Time: 6.70s\n",
      "[EP 9/1000] - Reward: -188.8093 - Steps: 234 - Eps: 0.0100 - Time: 6.61s\n",
      "[EP 10/1000] - Reward: -199.3090 - Steps: 339 - Eps: 0.0100 - Time: 9.54s\n",
      "[EP 11/1000] - Reward: -169.9914 - Steps: 534 - Eps: 0.0100 - Time: 15.16s\n",
      "[EP 12/1000] - Reward: -401.2338 - Steps: 882 - Eps: 0.0100 - Time: 24.86s\n",
      "[EP 13/1000] - Reward: -42.1594 - Steps: 530 - Eps: 0.0100 - Time: 14.93s\n",
      "[EP 14/1000] - Reward: -355.5482 - Steps: 850 - Eps: 0.0100 - Time: 24.29s\n",
      "[EP 15/1000] - Reward: -59.8155 - Steps: 1000 - Eps: 0.0100 - Time: 28.54s\n",
      "[EP 16/1000] - Reward: -55.0245 - Steps: 1000 - Eps: 0.0100 - Time: 28.50s\n",
      "[EP 17/1000] - Reward: -257.0254 - Steps: 793 - Eps: 0.0100 - Time: 22.92s\n",
      "[EP 18/1000] - Reward: -165.1817 - Steps: 530 - Eps: 0.0100 - Time: 15.25s\n",
      "[EP 19/1000] - Reward: -143.9743 - Steps: 1000 - Eps: 0.0100 - Time: 28.41s\n",
      "[EP 20/1000] - Reward: -65.1163 - Steps: 1000 - Eps: 0.0100 - Time: 28.27s\n",
      "[EP 21/1000] - Reward: -42.2127 - Steps: 1000 - Eps: 0.0100 - Time: 28.21s\n",
      "[EP 22/1000] - Reward: -67.8863 - Steps: 1000 - Eps: 0.0100 - Time: 28.33s\n",
      "[EP 23/1000] - Reward: -109.5272 - Steps: 1000 - Eps: 0.0100 - Time: 28.32s\n",
      "[EP 24/1000] - Reward: -12.6580 - Steps: 1000 - Eps: 0.0100 - Time: 28.04s\n",
      "[EP 25/1000] - Reward: -18.8279 - Steps: 1000 - Eps: 0.0100 - Time: 27.91s\n",
      "[EP 26/1000] - Reward: -64.5411 - Steps: 1000 - Eps: 0.0100 - Time: 28.14s\n",
      "[EP 27/1000] - Reward: -140.4186 - Steps: 792 - Eps: 0.0100 - Time: 22.06s\n",
      "[EP 28/1000] - Reward: -69.3879 - Steps: 100 - Eps: 0.0100 - Time: 2.78s\n",
      "[EP 29/1000] - Reward: -118.1653 - Steps: 97 - Eps: 0.0100 - Time: 2.65s\n",
      "[EP 30/1000] - Reward: -689.7309 - Steps: 177 - Eps: 0.0100 - Time: 4.88s\n",
      "[EP 31/1000] - Reward: -91.3200 - Steps: 635 - Eps: 0.0100 - Time: 17.72s\n",
      "[EP 32/1000] - Reward: -85.0770 - Steps: 847 - Eps: 0.0100 - Time: 23.32s\n",
      "[EP 33/1000] - Reward: -7.4210 - Steps: 1000 - Eps: 0.0100 - Time: 27.25s\n",
      "[EP 34/1000] - Reward: -51.1909 - Steps: 1000 - Eps: 0.0100 - Time: 27.45s\n",
      "[EP 35/1000] - Reward: -16.3621 - Steps: 1000 - Eps: 0.0100 - Time: 27.72s\n",
      "[EP 36/1000] - Reward: -41.1702 - Steps: 1000 - Eps: 0.0100 - Time: 27.69s\n",
      "[EP 37/1000] - Reward: -48.6096 - Steps: 1000 - Eps: 0.0100 - Time: 27.75s\n",
      "[EP 38/1000] - Reward: -77.5002 - Steps: 1000 - Eps: 0.0100 - Time: 27.68s\n",
      "[EP 39/1000] - Reward: -228.9448 - Steps: 747 - Eps: 0.0100 - Time: 20.31s\n",
      "[EP 40/1000] - Reward: -33.9483 - Steps: 1000 - Eps: 0.0100 - Time: 27.26s\n",
      "[EP 41/1000] - Reward: -137.6789 - Steps: 1000 - Eps: 0.0100 - Time: 27.21s\n",
      "[EP 42/1000] - Reward: 7.8224 - Steps: 1000 - Eps: 0.0100 - Time: 27.35s\n",
      "[EP 43/1000] - Reward: -135.9116 - Steps: 249 - Eps: 0.0100 - Time: 6.91s\n",
      "[EP 44/1000] - Reward: -23.9171 - Steps: 1000 - Eps: 0.0100 - Time: 27.44s\n",
      "[EP 45/1000] - Reward: -47.9047 - Steps: 1000 - Eps: 0.0100 - Time: 27.53s\n"
     ]
    }
   ],
   "source": [
    "agent = DuelingDQL(env, learning_rate, discount_factor, exploration_rate)\n",
    "history = agent.train(episodes, max_steps, log_wandb=True, update=True, save_episodes=True, save_interval=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test agent\n",
    "# history = agent.train(10, 1000, log_wandb=False, update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history\n",
    "if not os.path.exists('./history'):\n",
    "        os.mkdir('./history')\n",
    "\n",
    "with open('./history/dueling_dql_history.json', 'w') as file:\n",
    "    json.dump(history, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "666a1b6055a6d64029d3ba184b27518fce88fba32d5a3ee2ba82c9dc2fa1e9d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
