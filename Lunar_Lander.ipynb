{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DELE CA2 Part B Submission*\n",
    "\n",
    "# Lunar Lander: Learning to land a rocket! \n",
    "\n",
    "|          Name        |      Class    | Admin No. |\n",
    "|----------------------|---------------|-----------|\n",
    "| Timothy Chia Kai Lun | DAAA/FT/2B/02 | P2106911  |\n",
    "|      Lim Jun Jie     | DAAA/FT/2B/02 | P2100788  |\n",
    "\n",
    "![](https://i.imgur.com/2pzRsfx.jpg)\n",
    "\n",
    "**<u>Objectives</u>**\n",
    "\n",
    "We are tasked with training an agent capable of landing the lunar lander safely onto a landing pad [as per documentation](#https://gymnasium.farama.org/environments/box2d/lunar_lander/), using reinforcement learning techniques. The environment is provided to us through the [gymnasium library](#https://gymnasium.farama.org/).\n",
    "\n",
    "We aim to study the different behaviours executed in the Lunar Lander environment and attempt to optimize these behaviours such that we maximize on rewards.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Setup\n",
    "\n",
    "We have written modules for the algorithms used in this assignment as Python scripts including utility classes. These scripts are located in the `.\\models` directory. The rendering of episodes will be stored in .gif and can be found in the `.\\gifs` directory. \n",
    "\n",
    "For the deep learning aspect of the asignment, we will be using TensorFlow 2. Weights of the model upon completion of training and history of metrics can be found in `.\\assets` and `.\\history` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.dqn import DQN\n",
    "from models.sarsa import SARSA\n",
    "from models.dueling_dql import DuelingDQL\n",
    "from models.utils import ReplayBuffer, EpisodeSaver"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. About The Environment\n",
    "\n",
    "The Lunar Lander is a very interesting environment as it is described as a classic rocket trajectory optimization problem. The environment comes in two versions: discrete and continuous, with differing action spaces and the goal is to safely land the lunar lander on the launch pad located at (0,0).\n",
    "\n",
    "For this group assignment, we will be applying different RL algorithms on the discrete version environment which contain 4 actions in the action space and the state space is represented as a vector.\n",
    "\n",
    "### 1.1 States and Actions\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{State Vector}\n",
    "\\begin{cases}\n",
    "    s_0=\\text{x-axis coord of agent} \\\\\n",
    "    s_1=\\text{y-axis coord of agent} \\\\\n",
    "    s_2=\\text{x-axis linear velocity} \\\\\n",
    "    s_3=\\text{y-axis linear velocity} \\\\\n",
    "    s_4=\\text{Agent's angle} \\\\\n",
    "    s_5=\\text{Agent's angular velocity} \\\\\n",
    "    s_6=\\text{Right leg touched ground} \\\\\n",
    "    s_7=\\text{Left leg touched ground}\n",
    "\\end{cases}\n",
    "\n",
    "\\text{Action Space}\n",
    "\\begin{cases}\n",
    "    a_0=\\text{Do nothing} \\\\\n",
    "    a_1=\\text{Fire left engine} \\\\\n",
    "    a_2=\\text{Fire main engine} \\\\\n",
    "    a_3=\\text{Fire right engine}\n",
    "\\end{cases}\n",
    "\n",
    "$$\n",
    "\n",
    "### 1.2 Reward Scheme\n",
    "\n",
    "- The agent gains 100-140 points for landing on the launch pad and coming to a rest.\n",
    "- Coming to a rest yields an additional 100 points.\n",
    "- Each leg with ground contact earns an 10 points.\n",
    "- Moving away from landing spot decreases the rewards.\n",
    "- Crashing decreases the rewards by -100 points.\n",
    "- Firing the main engine decreases rewards by -0.3 points.\n",
    "- Firing the side engine decreases rewards by -0.3 points.\n",
    "\n",
    "An episode is considered a solution when the episodic rewards obtained are greater than or equal to 200 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', continuous=False, render_mode='rgb_array')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Reinforcement Learning Algorithms\n",
    "\n",
    "In this assignment, we will be comparing and analysing the differences in application and performance of different reinforcement algorithms. \n",
    "\n",
    "Reinforcement learning problems can be thought of as Markov Decision Processes (MDPs). MDPs are a way of fomarlizing sequential decision making and acts as the basis for structuring problems solved in reinforcement learning.\n",
    "\n",
    "**<u>Components of an MDP</u>**\n",
    "- Agent\n",
    "- Environment\n",
    "- State\n",
    "- Action\n",
    "- Reward signal\n",
    "\n",
    "The decision maker, called an agent, interacts with the environment it's placed in. These interactions occur sequentially over time. At each time step, the agent will get some representation of the environment's state. Given this representation, the agent selects an action to take. The environment is then transitioned into a new state, and the agent is given a reward as a consequence of the previous action. \n",
    "\n",
    "A visual representation of actions, states, and rewards can be seen below:\n",
    "\n",
    "![](https://i.imgur.com/0icpGlu.png)\n",
    "\n",
    "*Iterative feedback loop (Bhatt, 2019)*\n",
    "\n",
    "The goal of an agent is to maximize cumulative future rewards and it's actions are defined in terms of a policy (probability distribution). There are two kinds of policies which we will be exploring:\n",
    "\n",
    "<!--- \n",
    "source: https://analyticsindiamag.com/reinforcement-learning-policy/\n",
    "source: https://towardsdatascience.com/on-policy-v-s-off-policy-learning-75089916bc2f\n",
    "-->\n",
    "\n",
    "**<u>On-policy</u>**\n",
    "\n",
    "On-policy learning algorithms optimize the same policy (target policy) being used to select actions (behavioural policy) following some strategy (epsilon greedy). This means both target and behavioural policies are one and the same. \n",
    "\n",
    "The policy that is used for updating and the policy used for acting is the same, unlike in Q-learning. SARSA (State-Action-Reward-State-Action) is an example of an on-policy learning algorithm that we will be implementing for this assignment.\n",
    "\n",
    "**<u>Off-policy</u>**\n",
    "\n",
    "Off-policy learning algorithms are different from on-policy methods because what is being learned/optimized is target policy and not the behavioural policy. Q-Learning is an example of an off-policy algorithm. We will be implementing Deep Q-Learning which is different from vanilla Q-Learning as a neural network is used to map the state vector to Q-values (state-action values).\n",
    "\n",
    "In other words, it estimates the reward for future actions and appends a value to the new state without actually following any greedy policy.\n",
    "\n",
    "### 3.1 Baseline: Deep Q-Learning (DQN)\n",
    "\n",
    "<!--\n",
    "https://stats.stackexchange.com/questions/249355/how-exactly-to-compute-deep-q-learning-loss-function\n",
    "-->\n",
    "\n",
    "Q-learning maintains a Q-table that is iteratively updated in order to maximize the reward for every state encountered. This strategy is captured by the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q_{*}(s,a)=E[R_{t+1}+\\gamma \\underset{a'}{\\operatorname{max}} Q_{*}(s',a')\\space|\\space s,a]\n",
    "$$\n",
    "\n",
    "The optimal state-action value is the current reward and the discounted maximum future reward possible. In the case of finite discrete input spaces, the number of Q values to compute would be finite and can be solved iteratively with dynamic programming:\n",
    "\n",
    "$$\n",
    "Q_{i+1}(s,a)=E[R_{t+1}+\\gamma \\underset{a'}{\\operatorname{max}} Q_{*}(s',a')\\space|\\space s,a],\\space\\underset{i→\\infty}{\\operatorname{lim}}\\space Q_i=Q_{*}\n",
    "$$\n",
    "\n",
    "However in the case of the Lunar Lander environment, the state representation are continuous vectors and maintaing a Q-table would need infinite space. This is where we use neural networks as function approximators to estimate the Q function:\n",
    "\n",
    "$$\n",
    "Q(s,a;\\theta)\\approx Q(s,a)\n",
    "$$\n",
    "\n",
    "The Deep Q-Network is trained by minimizing the difference between the predicted and actual Q-values (Mean-Squared Error) for the loss function:\n",
    "\n",
    "$$\n",
    "L_{i}(\\theta_i) = (\\overbrace{(R_{t+1} + \\gamma\\underset{a'}{\\operatorname{max}}Q(s',a';\\theta_{i-1}\\space|\\space s,a))}^{\\text {Target Q-Value}} \\space\\space - \\overbrace{Q(s,a;\\theta_{i})}^{\\text {Predicted Q-Value}})^2\n",
    "$$\n",
    "\n",
    "Throughout multiple iterations, the Q-network is smoothened and an optimal Q-function is learned. The following are a few common concepts employed across our DQN agents.\n",
    "\n",
    "#### 3.1.1 Epsilon-Greedy Strategy\n",
    "\n",
    "The idea of exploiting what the agent already knows versus exploring a random action is called the exploration-exploitation trade-off. When the agent explores, it can improve its current knowledge and gain better rewards in the long run. However, when it exploits, it gets more reward immediately, even if it is a sub-optimal behavior. As the agent can’t do both at the same time, there is a trade-off.\n",
    "\n",
    "Using the epsilon-greedy strategy, the agent has some chance to select the action with the highest estimated reward. This strategy dictates that a random action is taken with probability $\\epsilon$, and a network based action is taken otherwise. \n",
    "\n",
    "\n",
    "This approach for action selection can be implemented in Python with a simple conditional statement:\n",
    "\n",
    "```python\n",
    "if np.random.random() < epsilon:\n",
    "    # select action by exploration\n",
    "else:\n",
    "    # select action by exploitation\n",
    "```\n",
    "\n",
    "#### 3.1.2 Experience Replay\n",
    "\n",
    "Sequential states are strongly correlated as the actions the lander would take at any presetn state is stringly correlated to actions taken in the past/future. To break correlation, we store a batch of past experiences and randomly select a uniformly distributed sample from this batch.\n",
    "\n",
    "Our implementation of a replay buffer can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_length, state_size, action_size, is_sarsa=False):\n",
    "        self.is_sarsa = is_sarsa\n",
    "        self.memory_counter = 0\n",
    "        self.max_length = max_length\n",
    "        self.state_memory = np.zeros((self.max_length, state_size))\n",
    "        self.new_state_memory = np.zeros((self.max_length, state_size))\n",
    "        self.action_memory = np.zeros((self.max_length, action_size), dtype=np.int8)\n",
    "        if is_sarsa:\n",
    "            self.new_action_memory = np.zeros((self.max_length, action_size), dtype=np.int8)\n",
    "        self.reward_memory = np.zeros(self.max_length)\n",
    "        self.done_memory = np.zeros(self.max_length, dtype=np.float32)\n",
    "\n",
    "    def append(self, state, action, reward, new_state, done, new_action=None):\n",
    "        idx = self.memory_counter % self.max_length\n",
    "\n",
    "        self.state_memory[idx] = state\n",
    "        \n",
    "        actions = np.zeros(self.action_memory.shape[1])\n",
    "        actions[action] = 1.0\n",
    "        self.action_memory[idx] = actions\n",
    "\n",
    "        if self.is_sarsa:\n",
    "            new_actions = np.zeros(self.action_memory.shape[1])\n",
    "            new_actions[new_action] = 1.0\n",
    "            self.new_action_memory[idx] = new_actions\n",
    "\n",
    "        self.new_state_memory[idx] = new_state\n",
    "        self.reward_memory[idx] = reward\n",
    "        self.done_memory[idx] = 1 - done\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_memory = min(self.memory_counter, self.max_length)\n",
    "        sampled_batch = np.random.choice(max_memory, batch_size, replace=False)\n",
    "        \n",
    "        states= self.state_memory[sampled_batch]\n",
    "        actions = self.action_memory[sampled_batch]\n",
    "        rewards= self.reward_memory[sampled_batch]\n",
    "        new_states = self.new_state_memory[sampled_batch]\n",
    "        if self.is_sarsa:\n",
    "            new_actions = self.new_action_memory[sampled_batch]\n",
    "        dones = self.done_memory[sampled_batch]\n",
    "\n",
    "        if not self.is_sarsa:\n",
    "            return states, actions, rewards, new_states, dones \n",
    "        else:\n",
    "            return states, actions, rewards, new_states, new_actions, dones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Network Architecture\n",
    "\n",
    "Our Q-network contains an input layer expecting batches of state vectors which are of size 8. We use 2 fully connected hidden layers with 256 nodes and ReLU activation each. The output layer is a fully connected layer with 4 nodes that are linearly activated andrepresents the Q-values for each of the 4 actions given a state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnetwork = Sequential([\n",
    "    Dense(units=256, activation='relu', input_shape=(state_size,)),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dense(units=action_size, activation='linear')\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Hyperparameters\n",
    "\n",
    "For all models and experiments done in this assignment, we have chosen to train the agent for a total of 500 episodes and maximum of 1000 steps per episode.\n",
    "\n",
    "We experimented with higher and lower values of learning rate and discount factor. However, we chose to keep the exploration rate of 1 as we would want our agent to explore the evironment before starting to learn from past experiences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 DQN Agent Performance\n",
    "\n",
    "We observed that a lower learning rate and discount factor performed much worse even though there is a rising moving average as compared to  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](https://i.imgur.com/btxQCuM.png)\n",
    "\n",
    "DQN agent with learning rate of 0.0005 and discount factor of 0.99 scored an average of 200 points at about ...\n",
    "\n",
    "![](https://i.imgur.com/DOr5VzM.gif)\n",
    "![](https://i.imgur.com/0WlIP8H.gif)\n",
    "![](https://i.imgur.com/siOTuaI.gif)\n",
    "![](https://i.imgur.com/74KZTZv.gif)\n",
    "![](https://i.imgur.com/v8aL37Y.gif)\n",
    "![](https://i.imgur.com/jVwkNK1.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 State-Action-Reward-State-Action (SARSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dueling Deep Q-Learning (DDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Experiments\n",
    "\n",
    "### 4.1 Random Engine Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "## 7. References\n",
    "\n",
    "in-text\n",
    "\n",
    "(Sutton & Barto, 1992)\n",
    "\n",
    "(Plaat, 2022)\n",
    "\n",
    "- Sutton, R.S. and Barto, A.G. (1992) Reinforcement learning: An introduction, Reinforcement Learning: An Introduction. Available at: http://www.incompleteideas.net/book/the-book-2nd.html (Accessed: January 31, 2023). \n",
    "\n",
    "- Plaat, A. (2022) Deep Reinforcement Learning, a textbook, arXiv.org. Available at: https://arxiv.org/abs/2201.02135 (Accessed: January 31, 2023). \n",
    "\n",
    "- Bhatt, S. (2019) Reinforcement learning 101, Medium. Towards Data Science. Available at: https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292 (Accessed: February 5, 2023). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbb20fadfa16e03ebe385758df0b196997ed807c890aaec95b326082f430e8b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
