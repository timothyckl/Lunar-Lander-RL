{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DELE CA2 Part B Submission*\n",
    "\n",
    "# Lunar Lander: Learning to land a rocket! \n",
    "\n",
    "|          Name        |      Class    | Admin No. |\n",
    "|----------------------|---------------|-----------|\n",
    "| Timothy Chia Kai Lun | DAAA/FT/2B/02 | P2106911  |\n",
    "|      Lim Jun Jie     | DAAA/FT/2B/02 | P2100788  |\n",
    "\n",
    "![](https://i.imgur.com/2pzRsfx.jpg)\n",
    "\n",
    "**<u>Objectives</u>**\n",
    "\n",
    "We are tasked with training an agent capable of landing the lunar lander safely onto a landing pad [as per documentation](#https://gymnasium.farama.org/environments/box2d/lunar_lander/), using reinforcement learning techniques. The environment is provided to us through the [gymnasium library](#https://gymnasium.farama.org/).\n",
    "\n",
    "We aim to study the different behaviours executed in the Lunar Lander environment and attempt to optimize these behaviours such that we maximize on rewards.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. About The Environment\n",
    "\n",
    "The Lunar Lander is a very interesting environment as it is described as a classic rocket trajectory optimization problem. The environment comes in two versions: discrete and continuous, with differing action spaces and the goal is to safely land the lunar lander on the launch pad located at (0,0).\n",
    "\n",
    "For this group assignment, we will be applying different RL algorithms on the discrete version environment which contain 4 actions in the action space and the state space is represented as an 8th dimensional vector.\n",
    "\n",
    "### 1.1 States and Actions\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{State Vector}\n",
    "\\begin{cases}\n",
    "    s_0=\\text{x-axis coord of agent} \\\\\n",
    "    s_1=\\text{y-axis coord of agent} \\\\\n",
    "    s_2=\\text{x-axis linear velocity} \\\\\n",
    "    s_3=\\text{y-axis linear velocity} \\\\\n",
    "    s_4=\\text{Agent's angle} \\\\\n",
    "    s_5=\\text{Agent's angular velocity} \\\\\n",
    "    s_6=\\text{Right leg touched ground} \\\\\n",
    "    s_7=\\text{Left leg touched ground}\n",
    "\\end{cases}\n",
    "\n",
    "\\text{Action Space}\n",
    "\\begin{cases}\n",
    "    a_0=\\text{Do nothing} \\\\\n",
    "    a_1=\\text{Fire left engine} \\\\\n",
    "    a_2=\\text{Fire main engine} \\\\\n",
    "    a_3=\\text{Fire right engine}\n",
    "\\end{cases}\n",
    "\n",
    "$$\n",
    "\n",
    "### 1.2 Reward Scheme\n",
    "\n",
    "- The agent gains 100-140 points for landing on the launch pad and coming to a rest.\n",
    "- Coming to a rest yields an additional 100 points.\n",
    "- Each leg with ground contact earns an 10 points.\n",
    "- Moving away from landing spot decreases the rewards.\n",
    "- Crashing decreases the rewards by -100 points.\n",
    "- Firing the main engine decreases rewards by -0.3 points.\n",
    "- Firing the side engine decreases rewards by -0.3 points.\n",
    "\n",
    "An episode is considered a solution when the episodic rewards obtained are greater than or equal to 200 points.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Project Setup\n",
    "\n",
    "We have written modules for the algorithms used in this assignment as Python scripts including utility classes. These scripts are located in the `.\\models` directory. The rendering of episodes will be stored in .gif and can be found in the `.\\gifs` directory. \n",
    "\n",
    "For the deep learning aspect of the asignment, we will be using TensorFlow 2. Weights of the model upon completion of training and history of metrics can be found in `.\\assets` and `.\\history` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.dqn import DQN\n",
    "from models.sarsa import SARSA\n",
    "from models.dueling_dql import DuelingDQL\n",
    "from models.utils import ReplayBuffer, EpisodeSaver"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Reinforcement Learning Algorithms\n",
    "\n",
    "In this assignment, we will be comparing and analysing the differences in application and performance of different reinforcement algorithms. \n",
    "\n",
    "Reinforcement learning problems can be thought of as Markov Decision Processes (MDPs). MDPs are a way of fomarlizing sequential decision making and acts as the basis for structuring problems solved in reinforcement learning.\n",
    "\n",
    "**<u>Components of an MDP</u>**\n",
    "\n",
    "1. Agent\n",
    "2. Environment\n",
    "3. State\n",
    "4. Action\n",
    "5. Reward signal\n",
    "\n",
    "The decision maker, called an agent, interacts with the environment it's placed in. These interactions occur sequentially over time. At each time step, the agent will get some representation of the environment's state. Given this representation, the agent selects an action to take. The environment is then transitioned into a new state, and the agent is given a reward as a consequence of the previous action.\n",
    "\n",
    "The goal of an agent is to maximize cumulative future rewards and it's actions are defined in terms of a policy (probability distribution). There are two kinds of policies which we will be exploring:\n",
    "\n",
    "<!--- \n",
    "source: https://analyticsindiamag.com/reinforcement-learning-policy/\n",
    "source: https://towardsdatascience.com/on-policy-v-s-off-policy-learning-75089916bc2f\n",
    "-->\n",
    "\n",
    "**<u>On-policy</u>**\n",
    "\n",
    "On-policy learning algorithms optimize the same policy (target policy) being used to select actions (behavioural policy) following some strategy (epsilon greedy). This means both target and behavioural policies are one and the same. \n",
    "\n",
    "The policy that is used for updating and the policy used for acting is the same, unlike in Q-learning. SARSA (State-Action-Reward-State-Action) is an example of an on-policy learning algorithm that we will be implementing for this assignment.\n",
    "\n",
    "**<u>Off-policy</u>**\n",
    "\n",
    "Off-policy learning algorithms are different from on-policy methods because what is being learned/optimized is target policy and not the behavioural policy. Q-Learning is an example of an off-policy algorithm. We will be implementing Deep Q-Learning which is different from vanilla Q-Learning as a neural network is used to map the state vector to Q-values (state-action values).\n",
    "\n",
    "In other words, it estimates the reward for future actions and appends a value to the new state without actually following any greedy policy.\n",
    "\n",
    "### 3.1 Baseline: Deep Q-Learning (DQN)\n",
    "\n",
    "<!--\n",
    "https://stats.stackexchange.com/questions/249355/how-exactly-to-compute-deep-q-learning-loss-function\n",
    "-->\n",
    "\n",
    "Q-learning maintains a Q-table that is iteratively updated in order to maximize the reward for every state encountered. This strategy is captured by the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q_{*}(s,a)=E[R_{t+1}+\\gamma \\underset{a'}{\\operatorname{max}} Q_{*}(s',a')\\space|\\space s,a]\n",
    "$$\n",
    "\n",
    "The optimal state-action value is the current reward and the discounted maximum future reward possible. In the case of finite discrete input spaces, the number of Q values to compute would be finite and can be solved iteratively with dynamic programming:\n",
    "\n",
    "$$\n",
    "Q_{i+1}(s,a)=E[R_{t+1}+\\gamma \\underset{a'}{\\operatorname{max}} Q_{*}(s',a')\\space|\\space s,a],\\space\\underset{i\â†’\\infty}{\\operatorname{lim}}\\space Q_i=Q_{*}\n",
    "$$\n",
    "\n",
    "However in the case of the Lunar Lander environment, the state representation are continuous vectors and maintaing a Q-table would need infinite space. This is where we use neural networks as function approximators to estimate the Q function:\n",
    "\n",
    "$$\n",
    "Q(s,a;\\theta)\\approx Q(s,a)\n",
    "$$\n",
    "\n",
    "The Deep Q-Network is trained by minimizing the difference between the predicted and actual Q-values (Mean-Squared Error) for the loss function:\n",
    "\n",
    "$$\n",
    "L_{i}(\\theta_i) = (\\overbrace{(R_{t+1} + \\gamma\\underset{a'}{\\operatorname{max}}Q(s',a';\\theta_{i-1}\\space|\\space s,a))}^{\\text {Target Q-Value}} \\space\\space - \\overbrace{Q(s,a;\\theta_{i})}^{\\text {Predicted Q-Value}})^2\n",
    "$$\n",
    "\n",
    "#### 3.1.1 Epsilon Greedy\n",
    "\n",
    "```python\n",
    "if np.random.random() < epsilon:\n",
    "    # explore\n",
    "else:\n",
    "    # exploit\n",
    "```\n",
    "\n",
    "#### 3.1.2 Experience Replay\n",
    "\n",
    "#### 3.1.3 Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 State-Action-Reward-State-Action (SARSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dueling Deep Q-Learning (DDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Experiments\n",
    "\n",
    "### 4.1 Random Engine Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "## 7. References\n",
    "\n",
    "in-text\n",
    "\n",
    "(Sutton & Barto, 1992)\n",
    "\n",
    "(Plaat, 2022)\n",
    "\n",
    "- Sutton, R.S. and Barto, A.G. (1992) Reinforcement learning: An introduction, Reinforcement Learning: An Introduction. Available at: http://www.incompleteideas.net/book/the-book-2nd.html (Accessed: January 31, 2023). \n",
    "\n",
    "- Plaat, A. (2022) Deep Reinforcement Learning, a textbook, arXiv.org. Available at: https://arxiv.org/abs/2201.02135 (Accessed: January 31, 2023). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15c5baac91721a2e9125fc9e7830b5bd4b6688550f5daa42d124d7a05043362d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
