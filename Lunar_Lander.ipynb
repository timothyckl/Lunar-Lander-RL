{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DELE CA2 Part B Submission*\n",
    "\n",
    "# Lunar Lander: Learning to land a rocket! \n",
    "\n",
    "|          Name        |      Class    | Admin No. |\n",
    "|----------------------|---------------|-----------|\n",
    "| Timothy Chia Kai Lun | DAAA/FT/2B/02 | P2106911  |\n",
    "|      Lim Jun Jie     | DAAA/FT/2B/02 | P2100788  |\n",
    "\n",
    "![](https://i.imgur.com/2pzRsfx.jpg)\n",
    "\n",
    "**<u>Objectives</u>**\n",
    "\n",
    "We are tasked with training an agent capable of landing the lunar lander safely onto a landing pad [as per documentation](#https://gymnasium.farama.org/environments/box2d/lunar_lander/), using reinforcement learning techniques. The environment is provided to us through the [gymnasium library](#https://gymnasium.farama.org/).\n",
    "\n",
    "## 1. About The Environment\n",
    "\n",
    "The Lunar Lander is a very interesting environment as it is described as a classic rocket trajectory optimization problem. The environment comes in two versions: discrete and continuous, with differing action spaces and the goal is to safely land the lunar lander on the launch pad located at (0,0).\n",
    "\n",
    "For this group assignment, we will be applying different RL algorithms on the discrete version environment which contain 4 actions in the action space and the state space is represented as an 8th dimensional vector.\n",
    "\n",
    "### 1.1 States and Actions\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{State Vector}\n",
    "\\begin{cases}\n",
    "    s_0=\\text{x-axis coord of agent} \\\\\n",
    "    s_1=\\text{y-axis coord of agent} \\\\\n",
    "    s_2=\\text{x-axis linear velocity} \\\\\n",
    "    s_3=\\text{y-axis linear velocity} \\\\\n",
    "    s_4=\\text{Agent's angle} \\\\\n",
    "    s_5=\\text{Agent's angular velocity} \\\\\n",
    "    s_6=\\text{Right leg touched ground} \\\\\n",
    "    s_7=\\text{Left leg touched ground}\n",
    "\\end{cases}\n",
    "\n",
    "\\text{Action Space}\n",
    "\\begin{cases}\n",
    "    a_0=\\text{Do nothing} \\\\\n",
    "    a_1=\\text{Fire left engine} \\\\\n",
    "    a_2=\\text{Fire main engine} \\\\\n",
    "    a_3=\\text{Fire right engine}\n",
    "\\end{cases}\n",
    "\n",
    "$$\n",
    "\n",
    "### 1.2 Reward Scheme\n",
    "\n",
    "- The agent gains 100-140 points for landing on the launch pad and coming to a rest.\n",
    "- Coming to a rest yields an additional 100 points.\n",
    "- Each leg with ground contact earns an 10 points.\n",
    "- Moving away from landing spot decreases the rewards.\n",
    "- Crashing decreases the rewards by -100 points.\n",
    "- Firing the main engine decreases rewards by -0.3 points.\n",
    "- Firing the side engine decreases rewards by -0.3 points.\n",
    "\n",
    "An episode is considered a solution when the episodic rewards obtained are greater than or equal to 200 points.\n",
    "\n",
    "## 2. Project Setup\n",
    "\n",
    "## 3. Reinforcement Learning Algorithms\n",
    "\n",
    "### 3.1 Baseline: Deep Q-Learning (DQN)\n",
    "\n",
    "### 3.2 State-Action-Reward-State-Action (SARSA)\n",
    "\n",
    "### 3.3 Dueling Deep Q-Learning (DDQN)\n",
    "\n",
    "## 4. Experiments\n",
    "\n",
    "### 4.1 Windy Environment\n",
    "\n",
    "### 4.2 Random Engine Failure\n",
    "\n",
    "## 4. Evaluation\n",
    "\n",
    "## 5. Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13 (default, Oct 19 2022, 22:38:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15c5baac91721a2e9125fc9e7830b5bd4b6688550f5daa42d124d7a05043362d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
