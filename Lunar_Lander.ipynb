{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DELE CA2 Part B Submission*\n",
    "\n",
    "# Lunar Lander: Learning to land a rocket! \n",
    "\n",
    "|          Name        |      Class    | Admin No. |\n",
    "|----------------------|---------------|-----------|\n",
    "| Timothy Chia Kai Lun | DAAA/FT/2B/02 | P2106911  |\n",
    "|      Lim Jun Jie     | DAAA/FT/2B/02 | P2100788  |\n",
    "\n",
    "![](https://i.imgur.com/2pzRsfx.jpg)\n",
    "\n",
    "**<u>Objectives</u>**\n",
    "\n",
    "We are tasked with training an agent capable of landing the lunar lander safely onto a landing pad [as per documentation](#https://gymnasium.farama.org/environments/box2d/lunar_lander/), using reinforcement learning techniques. The environment is provided to us through the [gymnasium library](#https://gymnasium.farama.org/).\n",
    "\n",
    "We aim to study the different behaviours executed in the Lunar Lander environment and attempt to optimize these behaviours such that we maximize on rewards.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. About The Environment\n",
    "\n",
    "The Lunar Lander is a very interesting environment as it is described as a classic rocket trajectory optimization problem. The environment comes in two versions: discrete and continuous, with differing action spaces and the goal is to safely land the lunar lander on the launch pad located at (0,0).\n",
    "\n",
    "For this group assignment, we will be applying different RL algorithms on the discrete version environment which contain 4 actions in the action space and the state space is represented as an 8th dimensional vector.\n",
    "\n",
    "### 1.1 States and Actions\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{State Vector}\n",
    "\\begin{cases}\n",
    "    s_0=\\text{x-axis coord of agent} \\\\\n",
    "    s_1=\\text{y-axis coord of agent} \\\\\n",
    "    s_2=\\text{x-axis linear velocity} \\\\\n",
    "    s_3=\\text{y-axis linear velocity} \\\\\n",
    "    s_4=\\text{Agent's angle} \\\\\n",
    "    s_5=\\text{Agent's angular velocity} \\\\\n",
    "    s_6=\\text{Right leg touched ground} \\\\\n",
    "    s_7=\\text{Left leg touched ground}\n",
    "\\end{cases}\n",
    "\n",
    "\\text{Action Space}\n",
    "\\begin{cases}\n",
    "    a_0=\\text{Do nothing} \\\\\n",
    "    a_1=\\text{Fire left engine} \\\\\n",
    "    a_2=\\text{Fire main engine} \\\\\n",
    "    a_3=\\text{Fire right engine}\n",
    "\\end{cases}\n",
    "\n",
    "$$\n",
    "\n",
    "### 1.2 Reward Scheme\n",
    "\n",
    "- The agent gains 100-140 points for landing on the launch pad and coming to a rest.\n",
    "- Coming to a rest yields an additional 100 points.\n",
    "- Each leg with ground contact earns an 10 points.\n",
    "- Moving away from landing spot decreases the rewards.\n",
    "- Crashing decreases the rewards by -100 points.\n",
    "- Firing the main engine decreases rewards by -0.3 points.\n",
    "- Firing the side engine decreases rewards by -0.3 points.\n",
    "\n",
    "An episode is considered a solution when the episodic rewards obtained are greater than or equal to 200 points.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Project Setup\n",
    "\n",
    "We have written modules for the algorithms used in this assignment as Python scripts including utility classes. These scripts are located in the `.\\models` directory. The rendering of episodes will be stored in .gif and can be found in the `.\\gifs` directory. \n",
    "\n",
    "For the deep learning aspect of the asignment, we will be using TensorFlow 2. Weights of the model upon completion of training and history of metrics can be found in `.\\assets` and `.\\history` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.dqn import DQN\n",
    "from models.sarsa import SARSA\n",
    "from models.dueling_dql import DuelingDQL\n",
    "from models.utils import ReplayBuffer, EpisodeSaver"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Reinforcement Learning Algorithms\n",
    "\n",
    "In this assignment, we will be comparing and analysing the differences in application and performance of different reinforcement algorithms. \n",
    "\n",
    "The topic of reinforment learning consist of five elements:\n",
    "\n",
    "1. An agent\n",
    "2. A policy\n",
    "3. A reward signal\n",
    "4. A value function \n",
    "\n",
    "The goal of an agent is to maximize cumulative future rewards and it's actions are defined in terms of a policy (probability distribution). There are two kinds of policies which we will be exploring:\n",
    "\n",
    "source: https://analyticsindiamag.com/reinforcement-learning-policy/\n",
    "\n",
    "**<u>Off-policy</u>**\n",
    "\n",
    "\n",
    "\n",
    "In Q-Learning, the agent learns optimal policy with the help of a greedy policy and behaves using policies of other agents. Q-learning is called off-policy because the updated policy is different from the behavior policy, so Q-Learning is off-policy. In other words, it estimates the reward for future actions and appends a value to the new state without actually following any greedy policy.\n",
    "\n",
    "**<u>On-policy</u>**\n",
    "\n",
    "SARSA (state-action-reward-state-action) is an on-policy reinforcement learning algorithm that estimates the value of the policy being followed. In this algorithm, the agent grasps the optimal policy and uses the same to act. The policy that is used for updating and the policy used for acting is the same, unlike in Q-learning. This is an example of on-policy learning.\n",
    "\n",
    "### 3.1 Baseline: Deep Q-Learning (DQN)\n",
    "\n",
    "#### 3.1.1 Epsilon Greedy\n",
    "\n",
    "```python\n",
    "if np.random.random() < epsilon:\n",
    "    # explore\n",
    "else:\n",
    "    # exploit\n",
    "```\n",
    "\n",
    "#### 3.1.2 Experience Replay\n",
    "\n",
    "#### 3.1.3 Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 State-Action-Reward-State-Action (SARSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dueling Deep Q-Learning (DDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Experiments\n",
    "\n",
    "### 4.1 Random Engine Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "## 7. References\n",
    "\n",
    "in-text\n",
    "\n",
    "(Sutton & Barto, 1992)\n",
    "\n",
    "(Plaat, 2022)\n",
    "\n",
    "- Sutton, R.S. and Barto, A.G. (1992) Reinforcement learning: An introduction, Reinforcement Learning: An Introduction. Available at: http://www.incompleteideas.net/book/the-book-2nd.html (Accessed: January 31, 2023). \n",
    "\n",
    "- Plaat, A. (2022) Deep Reinforcement Learning, a textbook, arXiv.org. Available at: https://arxiv.org/abs/2201.02135 (Accessed: January 31, 2023). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13 (default, Oct 19 2022, 22:38:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15c5baac91721a2e9125fc9e7830b5bd4b6688550f5daa42d124d7a05043362d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
